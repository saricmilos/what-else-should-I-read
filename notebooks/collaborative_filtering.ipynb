{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb23eb0",
   "metadata": {},
   "source": [
    "# **Book Recommendation System — Collaborative Filtering Approach**\n",
    "\n",
    "**Author:** Milos Saric [https://saricmilos.com/]  \n",
    "**Date:** November 04, 2025 - November 18th, 2025  \n",
    "**Dataset:** Kaggle — *Book Recommendation Dataset*  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe257b",
   "metadata": {},
   "source": [
    "### Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7dd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.dataloader import load_all_csvs_from_folder\n",
    "from src.preprocess_user_books_ratings import preprocess_books_ratings_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab81219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b535909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.user_cf import UserBasedCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8856d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.item_cf import (\n",
    "    filter_users_items,\n",
    "    train_test_split_by_user,\n",
    "    CFEncoders,\n",
    "    build_item_user_matrix,\n",
    "    compute_item_similarity,\n",
    "    get_top_k_items,\n",
    "    predict_ratings,\n",
    "    evaluate_predictions,\n",
    "    recommend_for_user,\n",
    "    recommend_similar_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b6bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = Path(r\"C:\\Users\\Milos\\Desktop\\ESCAPE_9-5\\PYTHON\\GitHub_Kaggle_Projects\\what-else-should-I-read\\datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_all_csvs_from_folder(dataset_folder,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f411061",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = preprocess_books_ratings_users(\n",
    "    datasets[\"Books\"],\n",
    "    datasets[\"Ratings\"],\n",
    "    datasets[\"Users\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86555135",
   "metadata": {},
   "source": [
    "# **1. Collaborative Filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae56ba",
   "metadata": {},
   "source": [
    "For **collaborative filtering (CF)**, we only need the core **user–item interactions**: `user_id`, `isbn` or `book title encoded`, and `book_rating`.  \n",
    "Other Metadata like user demographics, or book averages aren’t used in standard CF models, though they can be incorporated in **hybrid approaches**.  \n",
    "\n",
    "CF works by learning **similarity patterns**:  \n",
    "- Users who rate similar books in similar ways  \n",
    "- Books that are rated similarly by similar users  \n",
    "\n",
    "Other columns (age, country, author_avg_rating, etc.) are **side information**. They are useful for:  \n",
    "- Content-based recommenders  \n",
    "- Hybrid models combining CF with features  \n",
    "- Analytics, fairness, or deeper insights  \n",
    "\n",
    "But for a **pure CF matrix**, only the core user–item-rating data is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f863c0",
   "metadata": {},
   "source": [
    "### Step 1: filter active users and reasonably-rated books:\n",
    "New users have no interaction history, so it’s hard to find similar users. New items have few or no ratings, making them difficult to recommend.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2465f8",
   "metadata": {},
   "source": [
    "Create new data frame that only consists of user-item ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_df = merged_df[['user_id', 'book_title', 'book_rating']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_user_ratings = 10\n",
    "min_book_ratings = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97366def",
   "metadata": {},
   "source": [
    "Groups the dataframe cf_df by each user_id. Count the number of ratings each user has made (how many books they rated).Groups the dataframe by book_title. Counts how many users rated each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user and book counts\n",
    "user_counts = cf_df.groupby('user_id')['book_rating'].count()\n",
    "book_counts = cf_df.groupby('book_title')['book_rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cf_df['user_id'].nunique())  # total users\n",
    "print(cf_df['book_title'].nunique())  # total books\n",
    "\n",
    "print(sum(user_counts >= min_user_ratings))  # users meeting threshold\n",
    "print(sum(book_counts >= min_book_ratings))  # books meeting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_df_filtered = cf_df.copy()\n",
    "\n",
    "while True:\n",
    "    # Count ratings per user and per book\n",
    "    user_counts = cf_df_filtered.groupby('user_id')['book_rating'].count()\n",
    "    book_counts = cf_df_filtered.groupby('book_title')['user_id'].count()\n",
    "    \n",
    "    # Keep users and books meeting thresholds\n",
    "    active_users = user_counts[user_counts >= min_user_ratings].index\n",
    "    active_books = book_counts[book_counts >= min_book_ratings].index\n",
    "    \n",
    "    filtered = cf_df_filtered[\n",
    "        cf_df_filtered['user_id'].isin(active_users) &\n",
    "        cf_df_filtered['book_title'].isin(active_books)\n",
    "    ]\n",
    "    \n",
    "    # Stop if dataframe does not change\n",
    "    if len(filtered) == len(cf_df_filtered):\n",
    "        break\n",
    "    \n",
    "    cf_df_filtered = filtered.copy()\n",
    "\n",
    "# Final checks\n",
    "print(\"Filtered dataframe shape:\", cf_df_filtered.shape)\n",
    "min_ratings_per_user = cf_df_filtered.groupby('user_id')['book_rating'].count().min()\n",
    "min_ratings_per_book = cf_df_filtered.groupby('book_title')['user_id'].count().min()\n",
    "\n",
    "print(f\"Minimum number of books rated by any user: {min_ratings_per_user}\")\n",
    "print(f\"Minimum number of ratings for any book: {min_ratings_per_book}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab90896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many ratings each user gave\n",
    "user_counts = cf_df_filtered.groupby('user_id')['book_rating'].count()\n",
    "\n",
    "# Count how many users gave exactly k ratings\n",
    "freq = user_counts.value_counts().sort_index()\n",
    "\n",
    "# Limit to a sensible range (e.g., up to 30 ratings)\n",
    "max_k = 30\n",
    "freq = freq.loc[freq.index <= max_k]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = sns.barplot(x=freq.index.astype(int), y=freq.values, palette='pastel')\n",
    "plt.xlabel('Number of ratings per user')\n",
    "plt.ylabel('Number of users')\n",
    "plt.title('How many users rated exactly k books')\n",
    "\n",
    "# Dynamically adjust y-axis to leave space for numbers\n",
    "plt.ylim(0, freq.values.max() * 1.15)\n",
    "\n",
    "# Annotate bars above each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,  # center of the bar\n",
    "        height + freq.values.max() * 0.01,  # small offset above the bar\n",
    "        f\"{int(height)}\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04429554",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d54db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many ratings each book received\n",
    "book_counts = cf_df_filtered.groupby('book_title')['book_rating'].count()\n",
    "\n",
    "# Count how many books received exactly k ratings\n",
    "freq_books = book_counts.value_counts().sort_index()\n",
    "\n",
    "# Optionally limit to a sensible range (e.g., up to 30 ratings)\n",
    "max_k = 30\n",
    "freq_books = freq_books.loc[freq_books.index <= max_k]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = sns.barplot(x=freq_books.index.astype(int), y=freq_books.values, palette='pastel')\n",
    "plt.xlabel('Number of ratings per book')\n",
    "plt.ylabel('Number of books')\n",
    "plt.title('How many books received exactly k ratings')\n",
    "\n",
    "# Dynamically adjust y-axis to leave space for numbers\n",
    "plt.ylim(0, freq_books.values.max() * 1.15)\n",
    "\n",
    "# Annotate bars above each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,  # center of the bar\n",
    "        height + freq_books.values.max() * 0.01,  # small offset above the bar\n",
    "        f\"{int(height)}\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_counts.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e0820",
   "metadata": {},
   "source": [
    "### Step 2: Per User Train/Test Split\n",
    "\n",
    "To evaluate the model realistically, we create a **train/test split for each user**. Each user should have some ratings in the train set (so the model can learn their preferences) and some ratings in the test set (so we can evaluate the recommendations).  \n",
    "\n",
    "It’s important to make this split **reproducible** by setting a random seed, ensuring consistent results across runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b2b27",
   "metadata": {},
   "source": [
    "1. **Setup**:  \n",
    "   - `test_frac = 0.2` sets 20% of each user’s ratings for testing.  \n",
    "   - A fixed random seed (`random_seed = 42`) ensures reproducibility.\n",
    "\n",
    "2. **Per-user splitting**:  \n",
    "   - The dataframe is grouped by `user_id`.  \n",
    "   - For each user, their rating indices are shuffled randomly.  \n",
    "   - The first `n_test` ratings go to the test set, the rest to the train set.  \n",
    "   - If a user has only one rating in train or test, it ensures they appear in **both sets** by moving one rating to train if needed.\n",
    "\n",
    "3. **Build train/test sets**:  \n",
    "   - `train_df` contains all training ratings.  \n",
    "   - `test_df` contains all test ratings.  \n",
    "   - Resetting the index keeps the data clean.\n",
    "\n",
    "4. **Outcome**:  \n",
    "   - Every user is present in both train and test.  \n",
    "   - Train and test rows can now be used to **train the CF model** and **evaluate recommendations** reliably.\n",
    "\n",
    "The final print statements confirm the number of rows and unique users in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "test_frac = 0.2\n",
    "random_seed = 42\n",
    "rng = np.random.RandomState(random_seed)\n",
    "\n",
    "train_idxs = []\n",
    "test_idxs = []\n",
    "\n",
    "# group by user and split each user’s ratings\n",
    "for user, group in cf_df_filtered.groupby('user_id'):\n",
    "    idxs = group.index.tolist()\n",
    "    rng.shuffle(idxs)\n",
    "    \n",
    "    n_test = max(1, int(round(test_frac * len(idxs))))\n",
    "    test_for_user = idxs[:n_test]\n",
    "    train_for_user = idxs[n_test:]\n",
    "    \n",
    "    # ensure user appears in train too\n",
    "    if len(train_for_user) == 0 and len(test_for_user) > 1:\n",
    "        train_for_user.append(test_for_user.pop())\n",
    "    \n",
    "    train_idxs.extend(train_for_user)\n",
    "    test_idxs.extend(test_for_user)\n",
    "\n",
    "# Create splits\n",
    "train_df = cf_df_filtered.loc[train_idxs].reset_index(drop=True)\n",
    "test_df = cf_df_filtered.loc[test_idxs].reset_index(drop=True)\n",
    "\n",
    "print(\"Train rows:\", len(train_df))\n",
    "print(\"Test rows :\", len(test_df))\n",
    "print(\"Unique users in train:\", train_df['user_id'].nunique())\n",
    "print(\"Unique users in test :\", test_df['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb308416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users\n",
    "train_users = set(train_df['user_id'])\n",
    "test_users  = set(test_df['user_id'])\n",
    "unseen_users = test_users - train_users\n",
    "print(f\"Unseen users in test: {len(unseen_users)}\")\n",
    "\n",
    "# Items\n",
    "train_items = set(train_df['book_title'])\n",
    "test_items  = set(test_df['book_title'])\n",
    "unseen_items = test_items - train_items\n",
    "print(f\"Unseen items in test: {len(unseen_items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label encoders\n",
    "user_encoder = preprocessing.LabelEncoder()\n",
    "item_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit on train set users/items\n",
    "train_df['user_idx'] = user_encoder.fit_transform(train_df['user_id'])\n",
    "train_df['book_idx'] = item_encoder.fit_transform(train_df['book_title'])\n",
    "\n",
    "# Apply same mapping to test set\n",
    "test_df['user_idx'] = user_encoder.transform(test_df['user_id'])\n",
    "test_df['book_idx'] = item_encoder.transform(test_df['book_title'])\n",
    "\n",
    "print(\"Unique users encoded:\", train_df['user_idx'].nunique())\n",
    "print(\"Unique books encoded:\", train_df['book_idx'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27adde55",
   "metadata": {},
   "source": [
    "# **1.1. User Based CF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6f32c",
   "metadata": {},
   "source": [
    "### Step 3: Build user-item MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8b120",
   "metadata": {},
   "source": [
    "Construct a sparse matrix using csr_matrix. Each entry in the matrix represents a rating a user gave to a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0625d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "num_users = train_df['user_idx'].nunique()\n",
    "num_items = train_df['book_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb679a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Matrix dimensions: {num_users} users x {num_items} books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e32ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sparse matrix\n",
    "train_matrix = csr_matrix(\n",
    "    (train_df['book_rating'], \n",
    "     (train_df['user_idx'], train_df['book_idx'])),\n",
    "    shape=(num_users, num_items)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ec0a9",
   "metadata": {},
   "source": [
    "We can calculate how sparse the user–item matrix is and shows a small example of it for visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density = proportion of filled ratings\n",
    "density = train_matrix.count_nonzero() / (num_users * num_items)\n",
    "print(f\"Matrix density: {density:.4f}\")\n",
    "\n",
    "# Show a small dense sample (for inspection only)\n",
    "train_dense_sample = train_df.pivot_table(\n",
    "    index='user_idx', columns='book_idx', values='book_rating'\n",
    ").fillna(0)\n",
    "train_dense_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4642954",
   "metadata": {},
   "source": [
    "### Step 4: Compute User-User Similarity Using Cosine Similarity (Top-K Neighbors)\n",
    "\n",
    "In collaborative filtering, each user can be represented as a **vector of their ratings**.  \n",
    "\n",
    "- The **length of a user vector** is the magnitude of their ratings.  \n",
    "  - A user who gives many high ratings has a longer vector.  \n",
    "  - A user who gives few or low ratings has a shorter vector.  \n",
    "\n",
    "- **Cosine similarity** ignores the vector length and focuses on the **angle between vectors**:  \n",
    "  - Measures how similar the **pattern of ratings** is between users.  \n",
    "  - Users with similar preferences across books are considered similar, even if one tends to rate higher or lower overall.  \n",
    "\n",
    "Using **top-K neighbors**, we focus only on the most similar users when making recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6743cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sim_matrix = cosine_similarity(train_matrix)\n",
    "np.fill_diagonal(user_sim_matrix, 0)  # remove self-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f60852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small subset to visualize (e.g., first 50 users)\n",
    "subset = user_sim_matrix[:50, :50]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(subset, cmap='viridis')\n",
    "plt.title(\"User-User Cosine Similarity (subset of 50 users)\")\n",
    "plt.xlabel(\"User Index\")\n",
    "plt.ylabel(\"User Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c915851",
   "metadata": {},
   "source": [
    "### Step 5: Choose K Nearest Neighbors\n",
    "\n",
    "For each user, select the **top-K most similar users** from the user-user similarity matrix. These neighbors are used to generate recommendations based on what similar users have rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "top_k_users = np.argsort(-user_sim_matrix, axis=1)[:, :k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df587b",
   "metadata": {},
   "source": [
    "### **Step 6.Predict Ratings**  \n",
    "   - For each user-item pair, calculate the predicted rating as a weighted average of the ratings from the top-k similar users.  \n",
    "   - The weights are the similarity scores between the target user and each neighbor.\n",
    "\n",
    "3. **Normalization**  \n",
    "   - Normalize by the sum of similarity scores to ensure the predicted ratings are scaled correctly.  \n",
    "   - If none of the neighbors rated the item, use a fallback such as the user's average rating or the global average.\n",
    "\n",
    "4. **Build Full Prediction Matrix**  \n",
    "   - Repeat this process for all users and all items.  \n",
    "   - Each entry in the matrix represents the estimated rating a user would give to a particular item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c192625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user means\n",
    "user_means = np.array(train_matrix.mean(axis=1)).flatten()  # shape: (num_users,)\n",
    "\n",
    "# Initialize prediction matrix\n",
    "pred_matrix = np.zeros((num_users, num_items))\n",
    "\n",
    "# Loop over each user\n",
    "for u in range(num_users):\n",
    "    # Top-k similar users\n",
    "    neighbors = top_k_users[u]\n",
    "    sim_scores = user_sim_matrix[u, neighbors]  # similarity values\n",
    "    \n",
    "    # Neighbor ratings\n",
    "    neighbor_ratings = train_matrix[neighbors, :].toarray()  # shape (k, num_items)\n",
    "    \n",
    "    # Neighbor means\n",
    "    neighbor_means = user_means[neighbors][:, np.newaxis]  # shape (k, 1)\n",
    "    \n",
    "    # Mean-centered ratings\n",
    "    mean_centered = neighbor_ratings - neighbor_means  # r_vi - bar_r_v\n",
    "    \n",
    "    # Weighted sum\n",
    "    weighted_sum = sim_scores @ mean_centered  # numerator\n",
    "    sim_sum = np.sum(np.abs(sim_scores))       # denominator\n",
    "    \n",
    "    # Add target user mean\n",
    "    pred_matrix[u, :] = user_means[u] + (weighted_sum / sim_sum if sim_sum != 0 else 0)\n",
    "\n",
    "# Optional: fill NaNs for safety\n",
    "pred_matrix = np.nan_to_num(pred_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccaac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset for visualization (e.g., first 50 users and 50 books)\n",
    "subset = pred_matrix[:50, :50]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(subset, cmap='viridis')\n",
    "plt.title(\"Predicted Ratings Heatmap (subset of 50 users x 50 books)\")\n",
    "plt.xlabel(\"Book Index\")\n",
    "plt.ylabel(\"User Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861529df",
   "metadata": {},
   "source": [
    " **Step 7:Recommendation and Evaluation**  \n",
    "   - Recommend items to users by selecting those with the highest predicted ratings.  \n",
    "   - Evaluate the model by comparing predicted ratings to actual ratings in a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214784ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_by_user(user_id, top_k=10):\n",
    "    \"\"\"\n",
    "    Recommend books for a user based on user-user similarity.\n",
    "    Returns the top_k books that similar users liked.\n",
    "    \"\"\"\n",
    "    # Encode the user\n",
    "    user_idx = user_encoder.transform([user_id])[0]\n",
    "\n",
    "    # Get similarity scores for this user\n",
    "    sims = user_sim_matrix[user_idx]\n",
    "\n",
    "    # Find top-k similar users (excluding the user themselves)\n",
    "    top_k_idx = np.argsort(-sims)[:top_k+1]\n",
    "    top_k_idx = [i for i in top_k_idx if i != user_idx][:top_k]\n",
    "\n",
    "    # Compute average ratings from similar users for each book\n",
    "    # Use the trained rating matrix (train_matrix)\n",
    "    user_ratings = train_matrix[top_k_idx].toarray().mean(axis=0)\n",
    "\n",
    "    # Get top books not yet rated by this user\n",
    "    already_rated = train_matrix[user_idx].toarray().flatten() > 0\n",
    "    user_ratings[already_rated] = -np.inf  # exclude already rated\n",
    "\n",
    "    top_books_idx = np.argsort(-user_ratings)[:top_k]\n",
    "\n",
    "    # Convert back to book titles\n",
    "    recommended_books = item_encoder.inverse_transform(top_books_idx)\n",
    "\n",
    "    return recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "recommended = recommend_books_by_user(243, top_k=10)\n",
    "print(recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466cff38",
   "metadata": {},
   "source": [
    "# Evaluating the Recommendation System\n",
    "\n",
    "The numbers here are not flattering. Precision and recall measure how well the system recommends books users actually like, while MAE, MSE, and RMSE measure how far off the predicted ratings are from the real ones.\n",
    "\n",
    "Looking at the top-k metrics, Precision@5 is only 0.0542. That means out of the five books the system thinks a user will like most, **on average, only about one in twenty is actually relevant**. Even increasing the recommendations to 20 only raises recall to 0.1116, meaning the system captures barely 11% of items the user actually rated. Precision keeps dropping as k increases, showing that recommending more items mostly adds irrelevant books. This is painfully low, though not surprising given how sparse book rating datasets are.\n",
    "\n",
    "The rating prediction errors are equally harsh. MAE of 7.3627 and RMSE of 7.6219 indicate that, on average, the predicted rating is off by more than 7 points—absolutely massive if the scale is, for example, 1–10. MSE of 58.0931 highlights that some predictions are catastrophically wrong. These numbers tell us that the model is essentially guessing in most cases. While it might pick up very rough trends, it is **terrible at predicting individual ratings** and fails to produce meaningful recommendations in practice.\n",
    "\n",
    "In short: this model works just enough to generate numbers, but it’s nowhere near useful for real users. Expect mostly garbage recommendations if deployed as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True and predicted ratings for the test set\n",
    "users = test_df['user_idx'].to_numpy()\n",
    "items = test_df['book_idx'].to_numpy()\n",
    "y_true = test_df['book_rating'].to_numpy()\n",
    "y_pred = pred_matrix[users, items]\n",
    "\n",
    "# Metrics\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MAE  = {mae:.4f}\")\n",
    "print(f\"MSE  = {mse:.4f}\")\n",
    "print(f\"RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_n(train_df, test_df, pred_matrix, N=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Computes Precision@N and Recall@N for all users.\n",
    "    threshold: rating threshold above which items are considered 'relevant'\n",
    "    \"\"\"\n",
    "    # Build user → set of items in train/test\n",
    "    train_items = train_df.groupby('user_idx')['book_idx'].apply(set).to_dict()\n",
    "    test_items  = (\n",
    "        test_df[test_df['book_rating'] >= threshold]\n",
    "        .groupby('user_idx')['book_idx']\n",
    "        .apply(set)\n",
    "        .to_dict()\n",
    "    )\n",
    "    \n",
    "    precisions, recalls = [], []\n",
    "    \n",
    "    for user, true_items in test_items.items():\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Predicted scores for this user\n",
    "        scores = pred_matrix[user].copy()\n",
    "        # Exclude training items\n",
    "        scores[list(train_items.get(user, []))] = -np.inf\n",
    "        \n",
    "        # Top-N predicted items\n",
    "        top_n_items = np.argsort(-scores)[:N]\n",
    "        \n",
    "        # Count overlap\n",
    "        hits = len(set(top_n_items) & true_items)\n",
    "        precisions.append(hits / N)\n",
    "        recalls.append(hits / len(true_items))\n",
    "    \n",
    "    # Average over all users\n",
    "    precision_at_n = np.mean(precisions) if precisions else 0.0\n",
    "    recall_at_n    = np.mean(recalls) if recalls else 0.0\n",
    "    \n",
    "    return precision_at_n, recall_at_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in [5, 10, 20]:\n",
    "    p, r = precision_recall_at_n(train_df, test_df, pred_matrix, N=N, threshold=4.0)\n",
    "    print(f\"Precision@{N}: {p:.4f} | Recall@{N}: {r:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a89533",
   "metadata": {},
   "source": [
    "**1.1. Module Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b63b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubcf = UserBasedCF(\n",
    "    min_user_ratings=10,\n",
    "    min_book_ratings=10,\n",
    "    k_neighbors=10,\n",
    "    random_seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24007b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = ubcf.filter_dataframe(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8907f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubcf.train_test_split(filtered_df, test_frac=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = ubcf.build_train_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be956b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubcf.compute_user_similarity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubcf.predict_ratings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 243\n",
    "recommendations = ubcf.recommend_books_by_user(user_id=user_id, top_k=10)\n",
    "print(f\"Top 10 recommendations for user {user_id}:\\n\", recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ubcf.evaluate_rmse_mae()\n",
    "print(\"Evaluation metrics:\", metrics)\n",
    "# Example output: {'MAE': 0.92, 'MSE': 1.41, 'RMSE': 1.19}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116651dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = ubcf.precision_recall_at_n(N=10, threshold=4.0)\n",
    "print(f\"Precision@10: {precision:.4f}, Recall@10: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e4947",
   "metadata": {},
   "source": [
    "# **1.2. Item Based CF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4dd2b",
   "metadata": {},
   "source": [
    "### Step 3: Build item-user MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efccf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "num_users = train_df['user_idx'].nunique()\n",
    "num_items = train_df['book_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730db12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_user_train_matrix = csr_matrix(\n",
    "    (train_df['book_rating'], \n",
    "     (train_df['book_idx'], train_df['user_idx'])),\n",
    "    shape=(num_items, num_users)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699935e",
   "metadata": {},
   "source": [
    "We can calculate how sparse the item-user matrix is and shows a small example of it for visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density = proportion of filled ratings\n",
    "density = item_user_train_matrix.count_nonzero() / (num_users * num_items)\n",
    "print(f\"Matrix density: {density:.4f}\")\n",
    "\n",
    "# Show a small dense sample (for inspection only)\n",
    "train_dense_sample = train_df.pivot_table(\n",
    "    index='book_idx', columns='user_idx', values='book_rating'\n",
    ").fillna(0)\n",
    "train_dense_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e112ba9",
   "metadata": {},
   "source": [
    "### Step 4: Compute Item-Item Similarity Using Cosine Similarity (Top-K Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd96300",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sim_matrix = cosine_similarity(item_user_train_matrix)\n",
    "np.fill_diagonal(item_sim_matrix, 0)  # remove self-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024683f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small subset to visualize (e.g., first 50 users)\n",
    "subset = item_sim_matrix[:50, :50]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(subset, cmap='viridis')\n",
    "plt.title(\"Book-Book Cosine Similarity (subset of 50 books)\")\n",
    "plt.xlabel(\"User Index\")\n",
    "plt.ylabel(\"User Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1191ef",
   "metadata": {},
   "source": [
    "### Step 5: Choose K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "top_k_items = np.argsort(-item_sim_matrix, axis=1)[:, :k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654faa42",
   "metadata": {},
   "source": [
    "**Step 6.Predict Ratings**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fab508",
   "metadata": {},
   "source": [
    "When we want predicted ratings for a specific user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items, num_users = item_user_train_matrix.shape\n",
    "item_means = np.array(item_user_train_matrix.mean(axis=1)).flatten()\n",
    "item_pred_matrix = np.zeros((num_users, num_items))\n",
    "\n",
    "for i in range(num_items):\n",
    "    neighbors = top_k_items[i]\n",
    "    sim_scores = item_sim_matrix[i, neighbors]  # (k,)\n",
    "    \n",
    "    # Convert sparse matrix slice to dense\n",
    "    neighbor_ratings_dense = item_user_train_matrix[neighbors, :].toarray()  # (k, num_users)\n",
    "    \n",
    "    neighbor_means = item_means[neighbors][:, np.newaxis]  # (k,1)\n",
    "    mean_centered = neighbor_ratings_dense - neighbor_means  # (k,num_users)\n",
    "    \n",
    "    # Mask for where users actually rated neighbor items\n",
    "    mask = neighbor_ratings_dense != 0\n",
    "    \n",
    "    # Weighted sum\n",
    "    weighted_sum = (sim_scores[:, np.newaxis] * mean_centered) * mask\n",
    "    sim_sum_per_user = np.sum(np.abs(sim_scores[:, np.newaxis] * mask), axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    pred = np.where(sim_sum_per_user != 0, weighted_sum.sum(axis=0) / sim_sum_per_user, 0)\n",
    "    item_pred_matrix[:, i] = item_means[i] + pred\n",
    "\n",
    "# Ensure no NaNs\n",
    "item_pred_matrix = np.nan_to_num(item_pred_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262afed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset for visualization (e.g., first 50 users and 50 books)\n",
    "subset = item_pred_matrix[:50, :50]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(subset, cmap='viridis')\n",
    "plt.title(\"Predicted Ratings Heatmap (subset of 50 users x 50 books)\")\n",
    "plt.xlabel(\"Book Index\")\n",
    "plt.ylabel(\"User Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5714710",
   "metadata": {},
   "source": [
    " **Step: Recommendation and Evaluation**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fa77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_for_user_item_based(user_id, user_encoder, item_encoder, item_pred_matrix, n=10):\n",
    "    \"\"\"\n",
    "    Recommend top-n books for a given user using the item-based prediction matrix.\n",
    "    \"\"\"\n",
    "    # Encode the user (ensure it's known)\n",
    "    user_idx = user_encoder.transform([user_id])[0]\n",
    "\n",
    "    # Get the user’s predicted ratings\n",
    "    user_preds = item_pred_matrix[user_idx]\n",
    "\n",
    "    # Get top-N item indices\n",
    "    top_items_idx = np.argsort(-user_preds)[:n]\n",
    "\n",
    "    # Decode item titles\n",
    "    recommended_books = item_encoder.inverse_transform(top_items_idx)\n",
    "\n",
    "    # Get corresponding predicted scores\n",
    "    predicted_scores = user_preds[top_items_idx]\n",
    "\n",
    "    # Return both titles and scores\n",
    "    return list(zip(recommended_books, predicted_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: recommend 10 books for user_id = 243\n",
    "user_id = 243\n",
    "recommendations = recommend_books_for_user_item_based(\n",
    "    user_id=user_id,\n",
    "    user_encoder=user_encoder,\n",
    "    item_encoder=item_encoder,\n",
    "    item_pred_matrix=item_pred_matrix,\n",
    "    n=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top 10 recommendations for user {user_id}:\\n\")\n",
    "for book, score in recommendations:\n",
    "    print(f\"{book} — predicted rating: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books_by_book(book_title, top_k=10):\n",
    "    # Encode book\n",
    "    book_idx = item_encoder.transform([book_title])[0]\n",
    "    \n",
    "    # Get similarity scores for this book\n",
    "    sims = item_sim_matrix[book_idx]\n",
    "    \n",
    "    # Top-k similar books (excluding the book itself)\n",
    "    top_k_idx = np.argsort(-sims)[:top_k+1]\n",
    "    top_k_idx = [i for i in top_k_idx if i != book_idx][:top_k]\n",
    "    \n",
    "    # Convert indices back to book titles\n",
    "    recommended_books = item_encoder.inverse_transform(top_k_idx)\n",
    "    \n",
    "    return recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb568866",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_books_by_book(\"The Da Vinci Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718634b7",
   "metadata": {},
   "source": [
    "# Evaluating the Item-Based Recommendation System\n",
    "\n",
    "\n",
    "Looking at the top-N metrics, Precision@5 is only 0.0742, which means that out of the five books recommended to a user, **less than one book on average is actually relevant**. Even when we increase N to 20, the system captures only about 16% of the relevant items in Recall@20. Precision drops as more items are recommended, showing that most of what the system suggests is irrelevant. This slight improvement over the user-based method shows that item-based CF captures some patterns in item similarity, but it is still extremely weak for meaningful recommendations. Sparse user ratings and limited overlap between items are major limiting factors.\n",
    "\n",
    "The rating prediction errors remain harsh. MAE of 7.1474 and RMSE of 7.4423 indicate that the predicted ratings are still **off by more than 7 points on average**, which is massive if your rating scale is, for example, 1–10. MSE of 55.3873 highlights that some predictions are catastrophically wrong. These numbers suggest that while the item-based method is slightly better at ranking relevant items than user-based CF, it still **fails to predict actual ratings accurately** and would provide mostly useless recommendations if deployed.\n",
    "\n",
    "In short: item-based CF is a small step forward, but it is still far from usable in practice. Most recommendations are irrelevant, and the predicted ratings are wildly inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user and item indices from the test set\n",
    "users = test_df['user_idx'].to_numpy()\n",
    "items = test_df['book_idx'].to_numpy()\n",
    "\n",
    "# True ratings\n",
    "y_true = test_df['book_rating'].to_numpy()\n",
    "\n",
    "# Predicted ratings from your item-based CF matrix\n",
    "y_pred = item_pred_matrix[users, items]\n",
    "\n",
    "# Compute metrics\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MAE  = {mae:.4f}\")\n",
    "print(f\"MSE  = {mse:.4f}\")\n",
    "print(f\"RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_n_book_to_book(test_df, item_sim_matrix, item_encoder, input_books, N=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Computes Precision@N and Recall@N for book-to-book recommendations.\n",
    "    \n",
    "    test_df: DataFrame with columns ['user_idx', 'book_idx', 'book_rating']\n",
    "    item_sim_matrix: item-item similarity matrix\n",
    "    item_encoder: fitted LabelEncoder for books\n",
    "    input_books: list of book titles to test\n",
    "    N: top-N recommendations\n",
    "    threshold: rating threshold to consider an item relevant\n",
    "    \"\"\"\n",
    "    precisions, recalls = [], []\n",
    "    \n",
    "    for book in input_books:\n",
    "        # Encode input book\n",
    "        book_idx = item_encoder.transform([book])[0]\n",
    "        \n",
    "        # Get similarity scores for this book\n",
    "        sims = item_sim_matrix[book_idx]\n",
    "        \n",
    "        # Top-N recommended items excluding the book itself\n",
    "        top_n_idx = np.argsort(-sims)[:N+1]\n",
    "        top_n_idx = [i for i in top_n_idx if i != book_idx][:N]\n",
    "        \n",
    "        # Find users who rated this book above threshold in the test set\n",
    "        relevant_users = set(test_df[(test_df['book_idx'] == book_idx) & \n",
    "                                     (test_df['book_rating'] >= threshold)]['user_idx'])\n",
    "        \n",
    "        if not relevant_users:\n",
    "            continue\n",
    "        \n",
    "        # For each recommended book, count how many of these relevant users also rated it above threshold\n",
    "        hits = 0\n",
    "        for rec_idx in top_n_idx:\n",
    "            rec_users = set(test_df[(test_df['book_idx'] == rec_idx) & \n",
    "                                    (test_df['book_rating'] >= threshold)]['user_idx'])\n",
    "            hits += len(relevant_users & rec_users)\n",
    "        \n",
    "        # Precision = hits / (N * len(relevant_users)) ?\n",
    "        precisions.append(hits / (N * len(relevant_users)))\n",
    "        recalls.append(hits / len(relevant_users))\n",
    "    \n",
    "    # Average over all input books\n",
    "    precision_at_n = np.mean(precisions) if precisions else 0.0\n",
    "    recall_at_n    = np.mean(recalls) if recalls else 0.0\n",
    "    \n",
    "    return precision_at_n, recall_at_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0faef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example list of books to test\n",
    "test_books = [\"The Hobbit\", \"Touching Evil\", \"Pride and Prejudice\"]\n",
    "\n",
    "# Compute Precision@N and Recall@N for these books\n",
    "precision, recall = precision_recall_at_n_book_to_book(\n",
    "    test_df=test_df,\n",
    "    item_sim_matrix=item_sim_matrix,\n",
    "    item_encoder=item_encoder,\n",
    "    input_books=test_books,\n",
    "    N=10,          # top 10 similar books\n",
    "    threshold=4.0  # consider ratings >= 4 as relevant\n",
    ")\n",
    "\n",
    "print(f\"Book-to-Book Precision@10: {precision:.4f}\")\n",
    "print(f\"Book-to-Book Recall@10: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b4154",
   "metadata": {},
   "source": [
    "**Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filter_users_items(\n",
    "    merged_df, \n",
    "    user_col='user_id', \n",
    "    item_col='book_title', \n",
    "    rating_col='book_rating', \n",
    "    min_user_ratings=10, \n",
    "    min_item_ratings=10\n",
    ")\n",
    "print(\"Filtered shape:\", filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split_by_user(\n",
    "    filtered_df, \n",
    "    test_frac=0.2, \n",
    "    random_seed=42, \n",
    "    user_col='user_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2558cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = CFEncoders()\n",
    "train_df = encoders.fit(train_df, user_col='user_id', item_col='book_title')\n",
    "test_df = encoders.transform(test_df, user_col='user_id', item_col='book_title')\n",
    "num_users = train_df['user_idx'].nunique()\n",
    "num_items = train_df['item_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8466b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_user_matrix = build_item_user_matrix(\n",
    "    train_df, \n",
    "    rating_col='book_rating', \n",
    "    num_items=num_items, \n",
    "    num_users=num_users\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfee525",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sim_matrix = compute_item_similarity(item_user_matrix)\n",
    "top_k_items = get_top_k_items(item_sim_matrix, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pred_matrix = predict_ratings(item_user_matrix, item_sim_matrix, top_k_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_predictions(test_df, item_pred_matrix, rating_col='book_rating')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfeae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 243\n",
    "recommendations = recommend_for_user(\n",
    "    user_id, \n",
    "    encoders.user_encoder, \n",
    "    encoders.item_encoder, \n",
    "    item_pred_matrix, \n",
    "    n=10\n",
    ")\n",
    "for book, score in recommendations:\n",
    "    print(f\"{book}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70385a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_title = \"The Hobbit\"\n",
    "similar_books = recommend_similar_items(\n",
    "    book_title, \n",
    "    encoders.item_encoder, \n",
    "    item_sim_matrix, \n",
    "    k=10\n",
    ")\n",
    "print(\"Similar books:\", similar_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = filter_users_items(merged_df, min_user_ratings=10, min_item_ratings=10)\n",
    "# train_df, test_df = train_test_split_by_user(filtered_df, test_frac=0.2)\n",
    "\n",
    "# # Fit encoders\n",
    "# encoders = CFEncoders()\n",
    "# train_df = encoders.fit(train_df)\n",
    "\n",
    "# # Build item-user matrix\n",
    "# num_items = train_df['item_idx'].nunique()\n",
    "# num_users = train_df['user_idx'].nunique()\n",
    "# item_user_matrix = build_item_user_matrix(train_df, num_items=num_items, num_users=num_users)\n",
    "\n",
    "# # Compute similarity\n",
    "# item_sim_matrix = compute_item_similarity(item_user_matrix)\n",
    "\n",
    "# # Save encoder and similarity matrix\n",
    "# with open(\"item_sim_matrix.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(item_sim_matrix, f)\n",
    "\n",
    "# with open(\"item_encoder.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(encoders.item_encoder, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_books (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
