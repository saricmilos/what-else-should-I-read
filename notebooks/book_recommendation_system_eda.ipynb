{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265c115e",
   "metadata": {},
   "source": [
    "# **Book Recommendation System — Collaborative Filtering & Content-Based Approaches**\n",
    "\n",
    "**Author:** Milos Saric [https://saricmilos.com/]  \n",
    "**Date:** November 04, 2025 - November 18th, 2025  \n",
    "**Dataset:** Kaggle — *Book Recommendation Dataset*  \n",
    "\n",
    "---\n",
    "\n",
    "This notebook explores the Kaggle Book Recommendation dataset to build intelligent book recommendation systems using both **collaborative filtering** and **content-based** techniques. \n",
    " \n",
    "**Objective:**  \n",
    "To develop a personalized **book recommendation system** capable of understanding user preferences and providing meaningful, data-driven suggestions by integrating collaborative and content-based approaches.\n",
    "The analysis will guide you through the complete data science workflow, including:\n",
    "\n",
    "1. **Problem Definition** – Define the recommendation goal: predicting user preferences and suggesting books they are most likely to enjoy. Establish appropriate evaluation metrics (Precision@K, Recall@K, RMSE, MAE for ratings prediction).  \n",
    "\n",
    "2. **Data Collection & Overview** – Load and inspect datasets including `Books`, `Users`, and `Ratings` from Kaggle. Understand data structure, relationships, and key features.  \n",
    "\n",
    "3. **Exploratory Data Analysis (EDA)** – Analyze rating distributions, user activity, popular books, and correlations between users and items. Visualize key insights through plots.\n",
    "\n",
    "4. **Data Preprocessing & Feature Engineering** – Clean and merge datasets, handle missing values, normalize user IDs, and extract relevant metadata.  \n",
    "\n",
    "5. **Model Development** –  \n",
    "   - **Collaborative Filtering:** User-based and item-based similarity models using cosine correlation. \n",
    "   - **Content-Based Filtering:** Use TF-IDF or embeddings on book metadata (titles, authors) to find similar books.  \n",
    "6. **Evaluation & Testing** – Evaluate recommendation quality using ranking-based metrics. Compare different approaches and interpret the top recommended books for sample users.  \n",
    "\n",
    "7. **Deployment & Future Work** – Outline strategies to deploy the recommendation engine (via FastAPI).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a5b2f",
   "metadata": {},
   "source": [
    "# **About Recommender Systems**\n",
    "\n",
    "Over the past few decades, platforms like YouTube, Amazon, and Netflix have made recommender systems an essential part of our online experience. Whether it’s suggesting movies to watch, products to buy, or articles to read, these systems help users discover content that matches their interests.\n",
    "\n",
    "In simple terms, a recommender system is an algorithm that predicts what a user might like based on their preferences and behavior. They play a key role in many industries, not only improving user experience but also driving significant revenue and helping companies stand out from the competition.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Problem Definition**\n",
    "\n",
    "The first step is to establish a clear understanding of the challenge we aim to solve. This phase lays the groundwork for the entire project and ensures all subsequent efforts remain aligned with the primary objective.\n",
    "\n",
    "### **Objective**\n",
    "Develop a **book recommendation system** that predicts and suggests books users are most likely to enjoy, based on their past interactions, ratings, and preferences.  \n",
    "The system should intelligently recommend books by leveraging techniques such as **collaborative filtering** and **content-based filtering**.\n",
    "\n",
    "### **Scope**\n",
    "The analysis focuses on the Kaggle *Book Recommendation Dataset*, which includes three key components:  \n",
    "- **Users** – demographic and identification information.  \n",
    "- **Books** – metadata such as titles, authors, and publication details.  \n",
    "- **Ratings** – explicit user feedback (ratings from 1–10).  \n",
    "\n",
    "Predictions and recommendations are restricted to the dataset provided, without external sources unless explicitly integrated in advanced phases.\n",
    "\n",
    "### **Stakeholders**\n",
    "- **Readers / Users:** Receive personalized book suggestions based on reading habits and preferences.  \n",
    "- **Publishers & Authors:** Gain insights into reader interests, helping target audiences more effectively.  \n",
    "- **Data Scientists / ML Practitioners:** Experiment with various recommendation algorithms and performance optimization.  \n",
    "- **Platform Developers / Businesses:** Improve user engagement, sales, retention, and satisfaction through smarter recommendations.  \n",
    "\n",
    "### **Success Criteria**\n",
    "Achieve **high-quality and personalized recommendations**, evaluated using metrics such as:  \n",
    "- **Precision@K** and **Recall@K** — to measure recommendation relevance.  \n",
    "- **RMSE / MAE** — for explicit rating prediction accuracy.  \n",
    "\n",
    "A successful model will deliver **relevant, diverse, and accurate book suggestions** that enhance user experience and foster long-term engagement.\n",
    "\n",
    "> A clearly defined problem sets the foundation for building a meaningful and effective recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc1027",
   "metadata": {},
   "source": [
    "## **2. Data Collection**\n",
    "\n",
    "The **Data Collection** phase focuses on gathering and preparing the datasets required to build and evaluate the book recommendation models. This step also involves importing essential libraries, setting up the working environment, and organizing reusable functions to ensure a smooth analysis workflow.\n",
    "\n",
    "## **Dataset Description**\n",
    "\n",
    "The **Book-Crossing Dataset** comprises three main files: **Users**, **Books**, and **Ratings**.  \n",
    "Each file contains essential information used to build and evaluate the book recommendation system.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Users**\n",
    "\n",
    "Contains user-related information.  \n",
    "- **user_id** — Anonymized unique identifier for each user (mapped to integers).  \n",
    "- **location** — User’s location information (typically “City, State, Country”).  \n",
    "- **age** — User’s age (if available).  \n",
    "\n",
    "> Note: Some demographic fields may contain `NaN` values if data was unavailable.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Books**\n",
    "\n",
    "Contains metadata for each book.  \n",
    "- **isbn** — Unique book identifier (invalid isbns have already been removed).  \n",
    "- **book_title** — Title of the book.  \n",
    "- **book_author** — Author name (only the first author is provided in multi-author cases).  \n",
    "- **year_of_publication** — Year the book was published.  \n",
    "- **publisher** — Publishing company.  \n",
    "- **image_url_s / M / L** — URLs linking to small, medium, and large cover images hosted on Amazon.\n",
    "\n",
    "> Content-based features such as book titles, authors, and publication years were obtained from **Amazon Web Services**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Ratings**\n",
    "\n",
    "Contains user–book interaction data.  \n",
    "- **user_id** — References a unique user.  \n",
    "- **isbn** — References a unique book.  \n",
    "- **book_rating** — Explicit or implicit feedback:  \n",
    "  - **1–10** → Explicit ratings (higher values indicate stronger preference).  \n",
    "  - **0** → Implicit feedback (user has interacted but not rated explicitly).\n",
    "\n",
    "---\n",
    "\n",
    "You can access the dataset through the following sources:\n",
    "\n",
    "- **1.** Download directly from the Github repository: [https://github.com/saricmilos/what-else-should-I-read/tree/main/datasets](https://github.com/saricmilos/what-else-should-I-read/tree/main/datasets)  \n",
    "- **2.** Download from Kaggle: [Book Recommendation Dataset — Kaggle](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data)  \n",
    "\n",
    "Both sources provide the same data, so you can use whichever is most convenient for your workflow.\n",
    "\n",
    "> This dataset serves as the backbone of the recommendation system, enabling the discovery of patterns between users and books, and supporting the development of intelligent, personalized recommendation algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ed0b7",
   "metadata": {},
   "source": [
    "We can begin by:\n",
    "1. Loading each dataset (`Users`, `Books`, `Ratings`) individually.  \n",
    "2. Performing exploratory data analysis (EDA) to understand distributions and missing values.  \n",
    "3. Merging the datasets to form a unified view of user–book interactions.  \n",
    "4. Building and evaluating different recommendation approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5cb23a",
   "metadata": {},
   "source": [
    "### Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Modules\n",
    "from src.dataloader import load_all_csvs_from_folder\n",
    "from src.preprocess_location import preprocess_location\n",
    "from src.missing_values import (\n",
    "    missing_values_heatmap,\n",
    "    missing_values_barchart,\n",
    "    get_missing_value_summary\n",
    "    )\n",
    "from src.unique_values import (\n",
    "    get_column_types,\n",
    "    plot_number_of_unique_values,\n",
    "    unique_values\n",
    "    )\n",
    "from src.plots import (\n",
    "    plot_top_categories,\n",
    "    plot_boxplot,\n",
    "    plot_histogram\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa044f06",
   "metadata": {},
   "source": [
    "We begin by loading our datasets and cleaning column names for consistency.  \n",
    "Specifically, we:\n",
    "\n",
    "- Remove leading and trailing spaces\n",
    "- Convert all names to lowercase\n",
    "- Replace dashes (`-`) with underscores (`_`)\n",
    "\n",
    "This makes the DataFrame easier to work with in subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac57d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = Path(r\"C:\\Users\\Milos\\Desktop\\ESCAPE_9-5\\PYTHON\\GitHub_Kaggle_Projects\\what-else-should-I-read\\datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add36ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_all_csvs_from_folder(dataset_folder,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{datasets.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f86ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df, ratings_df, users_df = (datasets.get(key) for key in [\"Books\", \"Ratings\", \"Users\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = datasets.get(\"Books\")\n",
    "ratings_df = datasets.get(\"Ratings\")\n",
    "users_df = datasets.get(\"Users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3da46c",
   "metadata": {},
   "source": [
    "We remove leading/trailing spaces, convert all names to lowercase, and replace dashes with underscores. This makes the DataFrame easier to work with in subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0031ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [books_df, ratings_df, users_df]:\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()            # Remove leading/trailing whitespace\n",
    "        .str.lower()            # Convert all column names to lowercase\n",
    "        .str.replace('-', '_')  # Replace hyphens with underscores\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0b81a",
   "metadata": {},
   "source": [
    "##  **3. Exploratory Data Analysis (EDA)**\n",
    "\n",
    "Exploratory Data Analysis is all about **understanding the dataset**, uncovering patterns, spotting anomalies, and generating insights that will guide feature engineering and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e243f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = pd.DataFrame({\n",
    "    \"Dataset\": [\"books_df\", \"ratings_df\",\"users_df\"],\n",
    "    \"Shape\": [books_df.shape, ratings_df.shape,users_df.shape]\n",
    "})\n",
    "print(shapes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae0eea",
   "metadata": {},
   "source": [
    "# **3.1. Books Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc2647",
   "metadata": {},
   "source": [
    "### **3.1.1. Basic Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_books_df = books_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d49616",
   "metadata": {},
   "source": [
    "- There are **271,360 book entries** total.\n",
    "- The `isbn` field has only unique values as rows, meaning each book is uniquely identified by its ISBN.\n",
    "- `book_title` is *not unique* — only 242,135 unique titles, meaning some books share the same title (likely due to different editions). Selected Poems appear 27 times with different ISBN.\n",
    "- `book_author` has 102,022 unique names indicating several authors wrote multiple books.\n",
    "- The `year_of_publication` column has **202 unique years**, with **2002** being the most frequent (13,903 books).\n",
    "- The `publisher` column has **16,807 unique publishers**, with **Harlequin** being the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13121c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbcad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445921f",
   "metadata": {},
   "source": [
    "We can remove columns which contain book covers link images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059af124",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_cover_image_urls = books_df[[\"image_url_s\", \"image_url_m\", \"image_url_l\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df.drop(columns= [\"image_url_s\",\"image_url_m\",\"image_url_l\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ee063",
   "metadata": {},
   "source": [
    "### **3.1.2. Missing Values Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts_books = books_df.isna().sum().reset_index()\n",
    "na_counts_books.columns = ['Feature', 'MissingValues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{na_counts_books}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_value_summary(books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_barchart(books_df,\"Books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701980f7",
   "metadata": {},
   "source": [
    "### **3.1.3. Unique Values Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beddaff",
   "metadata": {},
   "source": [
    "### Book Metadata Overview\n",
    "\n",
    "The dataset contains **271,360 books**, each described by several key attributes. All attributes are object type. Here’s what we can observe:\n",
    "\n",
    "- **`isbn`** – Every book has a unique ISBN, which makes it a perfect **unique identifier** for our dataset. \n",
    "- **`book_title`** – There are **242,135 unique titles** (about **89%** of total entries). This means most titles are unique, though some duplicates exist — likely due to **different editions or reissues** of the same book.  \n",
    "- **`book_author`** – Around **102,022 unique authors** are present, meaning each author has, on average, **2–3 books** in the dataset.  \n",
    "- **`publisher`** – Only **16,807 unique publishers** appear, or roughly **6%** of entries. This makes sense, since **many books come from the same publisher**.  \n",
    "- **`year_of_publication`** – There are just **202 distinct years**, which is expected because **many books share the same publication year**.\n",
    "\n",
    "---\n",
    "\n",
    "Overall, the dataset shows good consistency, **ISBNs are unique**, while other fields naturally repeat due to multiple books by the same authors, publishers, or publication years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_books, int_cols_books, float_cols_books = get_column_types(books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ff5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values(books_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac905fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_number_of_unique_values(books_df,cat_cols_books,\"Books Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e615a51",
   "metadata": {},
   "source": [
    "We further explored the dataset by plotting value counts and found that the authors with the most published books are Agatha Christie (632), William Shakespeare (567), Stephen King (524), Ann M. Martin (423), and Carolyn Keene (373). The publishers with the most books are Harlequin (7,535), Silhouette (4,220), Pocket (3,905), and Ballantine Books (3,783)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_categories(books_df, 'book_author', top_n=10, orientation='h', palette='rainbow_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_categories(books_df, 'publisher', top_n=10, orientation='h', palette='rainbow_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1bf52e",
   "metadata": {},
   "source": [
    "# **3.2. Ratings Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b140b1",
   "metadata": {},
   "source": [
    "### **3.2.1. Basic Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ratings_df = ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721bc6b",
   "metadata": {},
   "source": [
    "Our ratings dataset contains three columns: `user_id`, `isbn`, and `book_rating`, with approximately **1,149,780 entries**.  \n",
    "There are **no missing values**. The `user_id` column is just an identifier, so statistics like mean or standard deviation are not meaningful.\n",
    "\n",
    "The `book_rating` column ranges from **0** (book read but not rated) to **10**, with a **mean of 2.87** and a **standard deviation of 3.85**, indicating high variability.  \n",
    "About **50% of ratings are 0** and **75% are below 7**, showing a **sparse ratings matrix** typical of recommendation datasets.\n",
    "\n",
    "**Data types:**\n",
    "- `isbn` → categorical  \n",
    "- `user_id` → integer  \n",
    "- `book_rating` → integer  \n",
    "\n",
    "**Unique values:**\n",
    "- `isbn`: 340,556 (~30%)  \n",
    "- `user_id`: 105,283 (~9%)  \n",
    "- `book_rating`: 11 (0–10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f98e8b",
   "metadata": {},
   "source": [
    "### **3.2.2. Missing Values Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21215d35",
   "metadata": {},
   "source": [
    "Rating Dataframe has NO missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ffe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af9f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts_ratings = ratings_df.isna().sum().reset_index()\n",
    "na_counts_ratings.columns = ['Feature', 'MissingValues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19329140",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{na_counts_ratings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_value_summary(ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d03f0",
   "metadata": {},
   "source": [
    "### **3.2.3. Unique Values Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bd6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_ratings, int_cols_ratings, float_cols_ratings = get_column_types(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_number_of_unique_values(ratings_df,ratings_df.columns,\"Ratings Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[\"book_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940d8fd",
   "metadata": {},
   "source": [
    "The histogram shows the distribution of user ratings for books. A large number of users (716,109) did not provide a rating, which reflects real-life behavior—most people enjoy a product and move on without leaving an online review. While this is natural, it poses a challenge for collaborative recommendation systems, as unrated items result in zero-length vectors and sparse data. For user-based filtering, new users with few or no ratings make it hard to find similar users. For item-based filtering, new books with few ratings are difficult to recommend, leading to the cold start problem and sparse data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(ratings_df, 'book_rating', bins=11, title='Rating Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdf550",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_categories(ratings_df, 'book_rating', top_n=11, orientation='v', palette='rainbow_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbce5d",
   "metadata": {},
   "source": [
    "# **3.3. Users Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b39ae",
   "metadata": {},
   "source": [
    "### **3.3.1. Basic Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_users_df = users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d10bb",
   "metadata": {},
   "source": [
    "We start by preprocessing the `location` column in the user dataset.  \n",
    "User-entered locations are often messy — with typos, alternate spellings, subregions, or single-entry countries.  \n",
    "Our goal is to **standardize and simplify** these values.\n",
    "\n",
    "---\n",
    "\n",
    "###  Location Preprocessing Functions\n",
    "\n",
    "- **`split_location(df)`**  \n",
    "  Splits the `location` column into `city`, `state`, and `country`, trims whitespace, and fills missing values with `'unknown'`.\n",
    "\n",
    "- **`clean_country(df, country_mapping, region_mapping)`**  \n",
    "  Standardizes country names, groups rare countries into `'other'`, removes quotes and empty strings, maps each country to a region, and drops the original `country` column.\n",
    "\n",
    "- **`clean_city(df, top_n=50)`**  \n",
    "  Cleans city names, replaces invalid or missing entries with `'unknown'`, keeps only the top *N* frequent cities, labels the rest as `'other'`, and drops the original column.\n",
    "\n",
    "- **`clean_state(df, top_n=50)`**  \n",
    "  Similar to `clean_city`, but applied to states.\n",
    "\n",
    "- **`preprocess_location(df, ...)`**  \n",
    "  Runs all the above steps and returns cleaned columns:  \n",
    "  `city_clean`, `state_clean`, `country_clean`, and `region`.\n",
    "\n",
    "---\n",
    "\n",
    "Overall, this process transforms messy location data into standardized, manageable categories suitable for **analysis or modeling**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42943666",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_mapping = {\n",
    "    # USA variants\n",
    "    'usa': 'usa', 'u.s.a.': 'usa', 'us': 'usa', 'america': 'usa', 'u.s.a': 'usa',\n",
    "    'united states': 'usa', 'united states of america': 'usa', 'united state': 'usa', \n",
    "    'united statea': 'usa', 'u.s. of a.': 'usa', 'u.s>': 'usa', 'uusa': 'usa',\n",
    "    'usa now': 'usa', 'good old usa !': 'usa', 'good old u.s.a.': 'usa',\n",
    "    'usa (currently living in england)': 'usa', 'usa\"': 'usa', 'us virgin islands': 'usa',\n",
    "    'american samoa': 'usa', 'ca': 'usa', 'nyc': 'usa', 'fl': 'usa', 'tx': 'usa',\n",
    "    'arizona': 'usa', 'california': 'usa', 'new york': 'usa', 'massachusetts': 'usa',\n",
    "    'ohio': 'usa', 'colorado': 'usa', 'michigan': 'usa', 'virginia': 'usa',\n",
    "    'washington': 'usa', 'missouri': 'usa', 'pennsylvania': 'usa', 'illinois': 'usa',\n",
    "    'nevada': 'usa', 'florida': 'usa', 'north carolina': 'usa', 'south carolina': 'usa',\n",
    "    'west virginia': 'usa', 'maine': 'usa', 'minnesota': 'usa', 'montana': 'usa',\n",
    "    'new jersey': 'usa', 'hawaii': 'usa', 'alaska': 'usa', 'texas': 'usa',\n",
    "    'louisiana': 'usa', 'oh': 'usa', 'nj': 'usa', 'ny': 'usa', 'va': 'usa',\n",
    "    'pa': 'usa', 'arizona': 'usa', 'florida': 'usa', 'mi': 'usa', 'california': 'usa',\n",
    "    'anystate': 'usa', 'everywhere and anywhere': 'usa', 'land of the free': 'usa',\n",
    "    \n",
    "    # UK variants\n",
    "    'uk': 'united kingdom', 'u.k.': 'united kingdom', 'england': 'united kingdom',\n",
    "    'england uk': 'united kingdom', 'united kingdom': 'united kingdom', 'scotland': 'united kingdom',\n",
    "    'wales': 'united kingdom', 'northern ireland': 'united kingdom',\n",
    "    \n",
    "    # Germany variants\n",
    "    'germany': 'germany', 'deutschland': 'germany', 'germay': 'germany', 'deutsches reich': 'germany',\n",
    "    'baden-wuerttemberg': 'germany', 'baden-württemberg': 'germany', 'hessen': 'germany',\n",
    "    'rheinland-pfalz': 'germany', 'bayern': 'germany', 'berlin': 'germany',\n",
    "    \n",
    "    # Spain variants\n",
    "    'spain': 'spain', 'españa': 'spain', 'espana': 'spain', 'espã±a': 'spain', \n",
    "    'spain\"': 'spain', 'andalucia': 'spain', 'catalunya': 'spain', 'catalonia': 'spain',\n",
    "    'pais vasco': 'spain', 'valtesse': 'spain', 'laos': 'spain', 'aragon': 'spain',\n",
    "    \n",
    "    # Italy variants\n",
    "    'italy': 'italy', 'italia': 'italy', 'l`italia': 'italy', 'italien': 'italy',\n",
    "    'italy\"': 'italy', 'emilia romagna': 'italy', 'lazio': 'italy', 'sicilia': 'italy',\n",
    "    'veneto': 'italy', 'toscana': 'italy', 'piemonte': 'italy', 'roma': 'italy', 'milano': 'italy',\n",
    "    'prov. di milano': 'italy', 'trentino alto adige': 'italy', 'roma': 'italy',\n",
    "    \n",
    "    # France variants\n",
    "    'france': 'france', 'la france': 'france', 'france\"': 'france', 'ile de france': 'france',\n",
    "    'bourgogne': 'france', 'alsace': 'france',\n",
    "    \n",
    "    # Portugal variants\n",
    "    'portugal': 'portugal', 'portugal\"': 'portugal', 'alentejo': 'portugal',\n",
    "    'lisboa': 'portugal', 'porto': 'portugal', 'coimbra': 'portugal', 'azores': 'portugal',\n",
    "    \n",
    "    # China variants\n",
    "    'china': 'china', 'p.r.china': 'china', 'p.r. china': 'china', 'people`s republic of china': 'china',\n",
    "    'china people`s republic': 'china', 'cn': 'china', 'china\"': 'china', 'beijing': 'china',\n",
    "    'shanghai': 'china', 'liaoning': 'china', 'shanxi province': 'china', 'hubei province': 'china',\n",
    "    'hunan': 'china', 'guangdong': 'china', 'shandong province': 'china', 'hebei': 'china',\n",
    "    \n",
    "    # Australia variants\n",
    "    'australia': 'australia', 'autralia': 'australia', 'australii': 'australia',\n",
    "    'australian capital territory': 'australia', 'new south wales': 'australia', 'nsw': 'australia',\n",
    "    'victoria': 'australia', 'queensland': 'australia', 'western australia': 'australia',\n",
    "    'south australia': 'australia', 'tasmania': 'australia', 'canberra': 'australia',\n",
    "    \n",
    "    # India variants\n",
    "    'india': 'india', 'india\"': 'india', 'maharashtra': 'india', 'maharastra': 'india',\n",
    "    'jharkhand': 'india', 'punjab': 'india', 'tamil nadu': 'india', 'indiai': 'india',\n",
    "    \n",
    "    # Japan variants\n",
    "    'japan': 'japan', 'japan military': 'japan', 'japan\"': 'japan', 'tokyo': 'japan',\n",
    "    'osaka': 'japan', 'seoul korea': 'japan', 'shizuoka pref.': 'japan',\n",
    "    \n",
    "    # Misc / Other countries (examples)\n",
    "    'canada': 'canada', 'british columbia': 'canada', 'ontario': 'canada', 'newfoundland': 'canada',\n",
    "    'mexico': 'mexico', 'mexico\"': 'mexico', 'ciudad de mexico': 'mexico', 'nuevo leon': 'mexico',\n",
    "    'venezuela': 'venezuela', 'venezuela\"': 'venezuela', 'brazil': 'brazil', 'brasil': 'brazil',\n",
    "    'brazil\"': 'brazil', 'argentina': 'argentina', 'la argentina': 'argentina', 'argentina\"': 'argentina',\n",
    "    'germany\"': 'germany', 'deutschland\"': 'germany', 'spain\"': 'spain',\n",
    "    'south korea': 'south korea', 'south korea\"': 'south korea', 'korea': 'south korea', 's.corea': 'south korea',\n",
    "    \n",
    "    # Catch all N/A or unknown\n",
    "    'n/a': 'unknown', 'n/a\"': 'unknown', 'none': 'unknown', 'unknown': 'unknown', '\"': 'unknown', '-': 'unknown',\n",
    "    '.': 'unknown', '*': 'unknown',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ec57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapping = {\n",
    "    # North America\n",
    "    'usa': 'North America',\n",
    "    'canada': 'North America',\n",
    "    'mexico': 'North America',\n",
    "    'bahamas': 'North America',\n",
    "    'puerto rico': 'North America',\n",
    "    'bermuda': 'North America',\n",
    "    'dc': 'North America',\n",
    "    'oregon': 'North America',  # US state\n",
    "    'u.s.a': 'North America',\n",
    "    \n",
    "    # Central America & Caribbean\n",
    "    'costa rica': 'Central America',\n",
    "    'panama': 'Central America',\n",
    "    'guatemala': 'Central America',\n",
    "    'jamaica': 'Caribbean',\n",
    "    'trinidad and tobago': 'Caribbean',\n",
    "    'dominican republic': 'Caribbean',\n",
    "    'grenada': 'Caribbean',\n",
    "    'barbados': 'Caribbean',\n",
    "    \n",
    "    # South America\n",
    "    'brazil': 'South America',\n",
    "    'argentina': 'South America',\n",
    "    'chile': 'South America',\n",
    "    'colombia': 'South America',\n",
    "    'peru': 'South America',\n",
    "    'venezuela': 'South America',\n",
    "    'bolivia': 'South America',\n",
    "    'uruguay': 'South America',\n",
    "    'ecuador': 'South America',\n",
    "    'paraguay': 'South America',\n",
    "    'urugua': 'South America',  # typo\n",
    "    \n",
    "    # Europe\n",
    "    'united kingdom': 'Europe',\n",
    "    'germany': 'Europe',\n",
    "    'spain': 'Europe',\n",
    "    'italy': 'Europe',\n",
    "    'france': 'Europe',\n",
    "    'portugal': 'Europe',\n",
    "    'netherlands': 'Europe',\n",
    "    'switzerland': 'Europe',\n",
    "    'sweden': 'Europe',\n",
    "    'finland': 'Europe',\n",
    "    'belgium': 'Europe',\n",
    "    'ireland': 'Europe',\n",
    "    'poland': 'Europe',\n",
    "    'greece': 'Europe',\n",
    "    'romania': 'Europe',\n",
    "    'croatia': 'Europe',\n",
    "    'slovakia': 'Europe',\n",
    "    'czech republic': 'Europe',\n",
    "    'russia': 'Europe/Asia',\n",
    "    'yugoslavia': 'Europe',\n",
    "    'slovenia': 'Europe',\n",
    "    'luxembourg': 'Europe',\n",
    "    'hungary': 'Europe',\n",
    "    'iceland': 'Europe',\n",
    "    'andorra': 'Europe',\n",
    "    'ukraine': 'Europe',\n",
    "    'lithuania': 'Europe',\n",
    "    'latvia': 'Europe',\n",
    "    'malta': 'Europe',\n",
    "    'albania': 'Europe',\n",
    "    'macedonia': 'Europe',\n",
    "    'cyprus': 'Europe',\n",
    "    'belarus': 'Europe',\n",
    "    'estonia': 'Europe',\n",
    "    'austria': 'Europe',\n",
    "    'austria\"': 'Europe',  # in case of trailing quotes\n",
    "    \n",
    "    # Asia\n",
    "    'china': 'Asia',\n",
    "    'japan': 'Asia',\n",
    "    'south korea': 'Asia',\n",
    "    'taiwan': 'Asia',\n",
    "    'hong kong': 'Asia',\n",
    "    'india': 'Asia',\n",
    "    'malaysia': 'Asia',\n",
    "    'singapore': 'Asia',\n",
    "    'philippines': 'Asia',\n",
    "    'indonesia': 'Asia',\n",
    "    'pakistan': 'Asia',\n",
    "    'iran': 'Asia',\n",
    "    'thailand': 'Asia',\n",
    "    'vietnam': 'Asia',\n",
    "    'nepal': 'Asia',\n",
    "    'kazakhstan': 'Asia',\n",
    "    'afghanistan': 'Asia',\n",
    "    'brunei': 'Asia',\n",
    "    'u.a.e': 'Middle East',\n",
    "    'united arab emirates': 'Middle East',\n",
    "    'qatar': 'Middle East',\n",
    "    'kuwait': 'Middle East',\n",
    "    'bahrain': 'Middle East',\n",
    "    'oman': 'Middle East',\n",
    "    'iraq': 'Middle East',\n",
    "    'sri lanka': 'Asia',\n",
    "    'burma': 'Asia',\n",
    "    \n",
    "    # Africa\n",
    "    'south africa': 'Africa',\n",
    "    'ghana': 'Africa',\n",
    "    'egypt': 'Africa',\n",
    "    'algeria': 'Africa',\n",
    "    'kenya': 'Africa',\n",
    "    'zimbabwe': 'Africa',\n",
    "    'ethiopia': 'Africa',\n",
    "    'mozambique': 'Africa',\n",
    "    'benin': 'Africa',\n",
    "    'morocco': 'Africa',\n",
    "    'mauritius': 'Africa',\n",
    "    \n",
    "    # Oceania\n",
    "    'australia': 'Oceania',\n",
    "    'new zealand': 'Oceania',\n",
    "    'fiji': 'Oceania',\n",
    "    \n",
    "    # Other / Unknown / Misc\n",
    "    'unknown': 'Unknown',\n",
    "    'other': 'Unknown',\n",
    "    'antarctica': 'Other',\n",
    "    'east africa': 'Africa',\n",
    "    'caribbean sea': 'Other',\n",
    "    'euskal herria': 'Europe',  # Basque Country (Spain/France)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f98054",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = preprocess_location(users_df,country_mapping=country_mapping,region_mapping=region_mapping,top_cities= 50,top_states= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9086108",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = users_df.drop(columns=\"location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41048f",
   "metadata": {},
   "source": [
    "### User Description\n",
    "\n",
    "The dataset includes **278,858 total users**, but only **168,096 have a recorded age** — meaning around **110,000 users didn’t provide their age**.\n",
    "\n",
    "The **average user ID** (139,429.5) doesn’t carry much meaning since it’s just an identifier.  \n",
    "The **average age** is about **35 years**, with a **standard deviation of roughly 14 years**, suggesting a fairly wide age spread among users.\n",
    "\n",
    "Looking at the distribution:\n",
    "- The **youngest recorded age is 0**, which is clearly an invalid or missing entry.  \n",
    "- **25% of users are younger than 24**, and **half are younger than 32**.  \n",
    "- **75% of users are younger than 44**, meaning most users fall between their mid-20s and mid-40s.  \n",
    "- The **maximum recorded age is 244**, which is obviously an outlier or data error.\n",
    "\n",
    "Overall, the age data contains some missing and unrealistic values, but most users are in the **24–44 age range**, with an average age around **35**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8981f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd17495",
   "metadata": {},
   "source": [
    "### **3.3.2. Missing Values Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b70560",
   "metadata": {},
   "source": [
    "Most columns are complete, except for `age`, which has a significant number of missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts_users = users_df.isna().sum().reset_index()\n",
    "na_counts_users.columns = ['Feature', 'MissingValues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{na_counts_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ed0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_value_summary(users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_heatmap(users_df,\"Users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df[users_df['age'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6d238",
   "metadata": {},
   "source": [
    "### **3.3.3. Unique Values Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860c41e",
   "metadata": {},
   "source": [
    "- `user_id` is fully unique (100%), serving as a unique identifier for each user.  \n",
    "- `age` has 165 unique values (~0.06%), indicating outliers.  \n",
    "- `country_clean` has 131 unique entries (~0.05%), `city_clean` 51 (~0.02%), and `state_clean` 50 (~0.02%).  \n",
    "- `region` is the least varied with only 13 unique values (~0.00%).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_users, int_cols_users, float_cols_users = get_column_types(users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed688165",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values(users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_number_of_unique_values(users_df,users_df.columns,\"Users Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_number_of_unique_values(users_df,cat_cols_users,\"Users Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a9b93",
   "metadata": {},
   "source": [
    "### **3.3.3. Location Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df[\"country_clean\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5be3b",
   "metadata": {},
   "source": [
    "Most reader ratings come the USA leading at 139,599 ratings, followed by Canada (21,622), the United Kingdom (18,587), Germany (17,092), and Spain (13,313)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e111c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_color = sns.color_palette('pastel')\n",
    "explode = [0.1] + [0]*(min(5, len(users_df['country_clean'].value_counts())) - 1)  # highlight first slice\n",
    "\n",
    "users_df['country_clean'].value_counts().iloc[:5].plot(\n",
    "    kind='pie',\n",
    "    colors=palette_color,\n",
    "    autopct='%.1f%%',\n",
    "    explode=explode,\n",
    "    shadow=True,\n",
    "    startangle=90\n",
    ")\n",
    "plt.title('Top 5 Countries', fontweight='bold')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_categories(users_df, 'country_clean', top_n=10, orientation='h', palette='rainbow_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5223e8",
   "metadata": {},
   "source": [
    "Most users are grouped under the category 'Other' because, during preprocessing, all less-popular cities were combined into this category. Excluding 'Other', we can see that major cities dominate user ratings, with London (4,105), Barcelona (2,664), Toronto (2,342), Madrid (1,933), Sydney (1,884), Chicago (1,566), and New York (1,445) leading the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_color = sns.color_palette('pastel')\n",
    "explode = [0.1] + [0]*(min(5, len(users_df['city_clean'].value_counts())) - 1)  # highlight first slice\n",
    "\n",
    "users_df['city_clean'].value_counts().iloc[:5].plot(\n",
    "    kind='pie',\n",
    "    colors=palette_color,\n",
    "    autopct='%.1f%%',\n",
    "    explode=explode,\n",
    "    shadow=True,\n",
    "    startangle=90\n",
    ")\n",
    "plt.title('Top 5 Cities', fontweight='bold')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65144d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_color = sns.color_palette('pastel')\n",
    "explode = [0.1] + [0]*(min(5, len(users_df['city_clean'].value_counts())) - 1)  # highlight first slice\n",
    "\n",
    "users_df['city_clean'].value_counts().iloc[1:6].plot(\n",
    "    kind='pie',\n",
    "    colors=palette_color,\n",
    "    autopct='%.1f%%',\n",
    "    explode=explode,\n",
    "    shadow=True,\n",
    "    startangle=90\n",
    ")\n",
    "plt.title('Top 5 Cities without UNKNOWN', fontweight='bold')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dabf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_categories(users_df, 'city_clean', top_n=10, orientation='h', palette='rainbow_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbaab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cities = users_df['city_clean'].value_counts().iloc[1:11].index\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(\n",
    "    y='city_clean',\n",
    "    data=users_df,\n",
    "    order=top_cities,\n",
    "    hue='city_clean',  # assign hue to avoid FutureWarning\n",
    "    dodge=False,\n",
    "    palette='pastel',\n",
    "    legend=False       # hide redundant legend\n",
    ")\n",
    "plt.title('Top 10 Cities without OTHER', fontweight='bold')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679293",
   "metadata": {},
   "source": [
    "# **4. Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490df447",
   "metadata": {},
   "source": [
    "# **4.1. Books Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a943ac2",
   "metadata": {},
   "source": [
    "There are two books with missing authors: \"A+ Quiz Masters:01 Earth\" (ISBN 0751352497) and \"The Credit Suisse\n",
    "Guide to Managing Your Personal Finances\" (ISBN 9627982032). We looked up the correct author information online\n",
    "and called them as \"Dorling Kindersley Publishing Staff\" and \"Credit Suisse\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4294804",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[books_df['book_author'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59469c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.loc[books_df['isbn'] == '0751352497', 'book_author'] = 'Dorling Kindersley Publishing Staff'\n",
    "books_df.loc[books_df['isbn'] == '9627982032', 'book_author'] = 'Credit Suisse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72007b",
   "metadata": {},
   "source": [
    "Similarly, two books had missing publishers: \"Tyrant Moon\" (ISBN 193169656X) and \"Finders Keepers\" (ISBN\n",
    "1931696993). We searched for the correct publishers and updated them to \"Mundania Press LLC\" and \"Random\n",
    "House Publishing Group.\"\n",
    "After these corrections, all books now have complete information for both authors and publishers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[books_df['publisher'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.loc[books_df[\"book_title\"]==\"Tyrant Moon\",\"publisher\"] = \"Mundania Press LLC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.loc[books_df[\"book_title\"]==\"Finders Keepers\",\"publisher\"] = \"Random House Publishing Group\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527c01e",
   "metadata": {},
   "source": [
    "### Data Quality Issue: `year_of_publication`\n",
    "\n",
    "The `year_of_publication` column is a mess. It contains a mix of different types of values:\n",
    "\n",
    "- **Integers** ( `1999`, `2002`)  \n",
    "- **Strings that look like years** ( `\"2000\"`, `\"1998\"`)  \n",
    "- **Non-year strings**, such as publisher names like `\"DK Publishing Inc\"`  and `\"Gallimard\"`  \n",
    "- **Impossible or invalid years**, such as `0`, `1378`, `1806`, `2030`, and `2050`\n",
    "\n",
    "Interestingly, when we check for non-digit entries, we only find **about three \"bad\" rows**, even though there are **65,000+ string values** in total.  \n",
    "Most of those string entries are actually **numeric strings** (e.g., `\"1998\"`, `\"2003\"`, `\"0\"`), which will need to be converted to integers and cleaned up before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb96303",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[\"year_of_publication\"].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a673721",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[~books_df[\"year_of_publication\"].apply(lambda x: str(x).isdigit())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19434dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[\"year_of_publication\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930b77b1",
   "metadata": {},
   "source": [
    "To clean up the `year_of_publication` column, we can convert all values to numeric using `pd.to_numeric()` with the `errors='coerce'` parameter.  \n",
    "This will turn any invalid or non-numeric entries (like publisher names or impossible strings) into `NaN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64228cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[\"year_of_publication\"] = pd.to_numeric(books_df[\"year_of_publication\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21642913",
   "metadata": {},
   "source": [
    "Replaces invalid years (<1000 or >2025) in `year_of_publication` with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862dc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.loc[\n",
    "    (books_df[\"year_of_publication\"] < 1000) | \n",
    "    (books_df[\"year_of_publication\"] > 2025),\n",
    "    \"year_of_publication\"\n",
    "] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4ea85",
   "metadata": {},
   "source": [
    "Fill missing `year_of_publication` values with the median, then convert the column to integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[\"year_of_publication\"] = books_df[\"year_of_publication\"].fillna(\n",
    "    books_df[\"year_of_publication\"].median()\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f29917",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(books_df, 'year_of_publication', bins=300, title='Year of Publication Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(books_df[books_df[\"year_of_publication\"]>1950], 'year_of_publication', bins=50, title='Year of Publication Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51d1ea",
   "metadata": {},
   "source": [
    "We can keep only books from 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f882f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df[books_df['year_of_publication'].between(1900, 2025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1540b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(books_df, 'year_of_publication', bins=50, title='Year of Publication Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2cf8d5",
   "metadata": {},
   "source": [
    "# **4.2. Ratings Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12679a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[\"book_rating\"].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[\"isbn\"].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e63ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df[\"user_id\"].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4384745",
   "metadata": {},
   "source": [
    "# **4.3. Users Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3782961",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84a687",
   "metadata": {},
   "source": [
    "The `age` column is **not normally (Gaussian) distributed** — it is heavily skewed, with many extreme values. Simply relying on the standard Interquartile Range (IQR) method to remove outliers does not work well for this dataset.\n",
    "\n",
    "- **IQR-based bounds**:  \n",
    "  - Lower bound: `Q1 - 1.5*IQR = 24 - 30 = -6` → meaningless, leaves many ages as `0` or negative, which are invalid.  \n",
    "  - Upper bound: `Q3 + 1.5*IQR = 44 + 30 = 74` → too low, would remove valid older users (e.g., 75–99).  \n",
    "\n",
    "- **Conclusion**: IQR filtering alone is insufficient.  \n",
    "- **Explicit filtering is needed**: remove ages **below 5** and **above 99** to ensure realistic, meaningful values while keeping the majority of valid data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcab88",
   "metadata": {},
   "source": [
    "Normal Gauissan Distribution for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d50c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 random numbers from a normal distribution\n",
    "mu, sigma = 50, 10  # mean and standard deviation\n",
    "data = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "# Convert to a pandas Series for easy description\n",
    "data_series = pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"Descriptive statistics for the normal distribution:\")\n",
    "print(data_series.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fcc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(data, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Normal (Gaussian) Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bccde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,6))\n",
    "plt.boxplot(data, vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightgreen', color='green'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title(\"Boxplot of Normal (Gaussian) Distribution\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[\"year_of_publication\"].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(users_df, 'age', bins=30, title='age Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(users_df, 'age', title='Distribution of age', ylabel='age', palette='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 countries by count\n",
    "top_countries = users_df['country_clean'].value_counts().nlargest(10).index\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_top10 = users_df[users_df['country_clean'].isin(top_countries)]\n",
    "\n",
    "# Plot only the top 10 countries\n",
    "plot_boxplot(\n",
    "    df_top10, \n",
    "    column='age', \n",
    "    by='country_clean', \n",
    "    title='age by Top 10 Countries', \n",
    "    ylabel='age', \n",
    "    xlabel='Country', \n",
    "    palette='Set2', \n",
    "    rotate_xticks=True, \n",
    "    figsize=(12,6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfd191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly remove impossible ages\n",
    "users_df.loc[(users_df['age'] <= 5) | (users_df['age'] > 99), 'age'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(users_df,\"age\",bins = 30, title= \"Histogram of age after removing outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(users_df, 'age', title='Distribution of age', ylabel='age', palette='cool')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b3365",
   "metadata": {},
   "source": [
    "We still have a **large number of missing values**. This shows that **112,043 rows** have missing `age`. Dropping these rows would result in a huge data loss, so we need to **impute** them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225fad2",
   "metadata": {},
   "source": [
    "We use a **random imputation method** based on the statistical distribution of the existing ages, median(central tendency of the ages), std (spread of the ages), nulls (total number of missing age values):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f84291",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = users_df['age'].median()\n",
    "std = users_df['age'].std()\n",
    "nulls = users_df['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bd596",
   "metadata": {},
   "source": [
    "This simulates a realistic age distribution using the existing data, centers the distribution around the median age and ensures the generated ages have a similar spread as the original data and ensures ages are not below 5 or above 100, keeping them realistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fee99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_age = np.random.normal(loc=median, scale=std, size=nulls)\n",
    "random_age = np.clip(random_age, 5, 100)  # limit to plausible range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813cf6e",
   "metadata": {},
   "source": [
    "Assigns the generated random ages to the missing entries and rounds to integer values since age is discrete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.loc[users_df['age'].isna(), 'age'] = random_age.round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cfdea",
   "metadata": {},
   "source": [
    "Missing values are replaced with realistic ages, the overall age distribution remains similar to the original data, no extreme outliers are introduced, and all ages fall within the plausible range of 5–100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534708e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(users_df,\"age\",bins = 30, title= \"Histogram of age after removing outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(users_df, 'age', title='Distribution of age', ylabel='age', palette='cool')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726a915",
   "metadata": {},
   "source": [
    "# **5. Merge Datasets / Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd89cfd",
   "metadata": {},
   "source": [
    "Merges `users_df` (278,858 rows, 6 columns) with `ratings_df` (1,149,780 rows, 3 columns) on `user_id`.  \n",
    "Each row in the result represents a rating with the corresponding user’s info.  \n",
    "\n",
    "By default, `pd.merge()` does an **inner join**:\n",
    "\n",
    "- Keeps only `user_id`s present in both dataframes.  \n",
    "- Users without ratings or ratings without matching users are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ratings=pd.merge(users_df,ratings_df,on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092f453",
   "metadata": {},
   "source": [
    "When combining datasets, we merge/join on **`isbn`** rather than `book_title`.because **`isbn` is unique**: Each book edition has a distinct ISBN across all tables, and we determined in EDA **`book_title` IS NOT be unique**: Multiple editions of the same book share a title but have different content. Typos, extra spaces, or differences in casing can cause mismatches. **Merging on `book_title` can lead to:**  Duplicate rows, Incorrect joins, Missing records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964182b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=pd.merge(users_ratings,books_df,on='isbn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_merged_df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate mean and count of ratings per book\n",
    "book_rating = (\n",
    "    merged_df.groupby(['book_title', 'book_author'])['book_rating']\n",
    "    .agg(count='count', mean='mean')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter to only books with a substantial number of ratings\n",
    "filtered_books_500 = book_rating.query(\"count > 500\").sort_values('mean', ascending=False).head(10)\n",
    "\n",
    "# Improve the plot\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=filtered_books_500,\n",
    "    x='mean',\n",
    "    y='book_title',\n",
    "    hue='book_author',\n",
    "    palette='Paired',\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Average Rating', fontsize=12)\n",
    "plt.ylabel('Book Title', fontsize=12)\n",
    "plt.title('Top 10 Highly Rated Books (with >500 Ratings)', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_books_500.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c285c",
   "metadata": {},
   "source": [
    "We can remove ratings with a value of `0`, which represent books that users haven’t actually rated.  \n",
    "However, doing this reduces the number of ratings per book, so some books that originally had more than 500 ratings may now fall below that threshold after these rows are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42240b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['book_rating']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d30c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate mean and count of ratings per book\n",
    "book_rating = (\n",
    "    merged_df.groupby(['book_title', 'book_author'])['book_rating']\n",
    "    .agg(count='count', mean='mean')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter to only books with a substantial number of ratings\n",
    "filtered_books_no_zero_300 = book_rating.query(\"count > 300\").sort_values('mean', ascending=False).head(10)\n",
    "\n",
    "# Improve the plot\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=filtered_books_no_zero_300,\n",
    "    x='mean',\n",
    "    y='book_title',\n",
    "    hue='book_author',\n",
    "    palette='Paired',\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Average Rating', fontsize=12)\n",
    "plt.ylabel('Book Title', fontsize=12)\n",
    "plt.title('Top 10 Highly Rated Books (with >300 Ratings) no zero rating', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d25cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate mean and count of ratings per book\n",
    "book_rating = (\n",
    "    merged_df.groupby(['book_title', 'book_author'])['book_rating']\n",
    "    .agg(count='count', mean='mean')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter to only books with a substantial number of ratings\n",
    "filtered_books_no_zero_400 = book_rating.query(\"count > 400\").sort_values('mean', ascending=False).head(10)\n",
    "\n",
    "# Improve the plot\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=filtered_books_no_zero_400,\n",
    "    x='mean',\n",
    "    y='book_title',\n",
    "    hue='book_author',\n",
    "    palette='Paired',\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Average Rating', fontsize=12)\n",
    "plt.ylabel('Book Title', fontsize=12)\n",
    "plt.title('Top 10 Highly Rated Books (with >400 Ratings) no zero rating', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate mean and count of ratings per book\n",
    "book_rating = (\n",
    "    merged_df.groupby(['book_title', 'book_author'])['book_rating']\n",
    "    .agg(count='count', mean='mean')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter to only books with a substantial number of ratings\n",
    "filtered_books_no_zero_500 = book_rating.query(\"count > 500\").sort_values('mean', ascending=False).head(10)\n",
    "\n",
    "# Improve the plot\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=filtered_books_no_zero_500,\n",
    "    x='mean',\n",
    "    y='book_title',\n",
    "    hue='book_author',\n",
    "    palette='Paired',\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Average Rating', fontsize=12)\n",
    "plt.ylabel('Book Title', fontsize=12)\n",
    "plt.title('Top 10 Highly Rated Books (with >500 Ratings) no zero rating', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683afc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_books_no_zero_300.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06dbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_books_no_zero_400.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_books_no_zero_500.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(merged_df, 'book_rating', bins=10, title='Rating Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37d695",
   "metadata": {},
   "source": [
    "# **5.1. Feature Engineering and More Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ecdda4",
   "metadata": {},
   "source": [
    "This section creates **user-level features**:\n",
    "\n",
    "- **`user_avg_rating`** – the average rating each user gives  \n",
    "- **`user_num_ratings`** – the total number of ratings per user  \n",
    "- **`User_rating_variability`** – how consistent a user’s ratings are (standard deviation)  \n",
    "\n",
    "These features are stored in a separate dataframe for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7969da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating given by each user\n",
    "merged_df.loc[:, 'user_avg_rating'] = merged_df.groupby('user_id')['book_rating'].transform('mean')\n",
    "\n",
    "# Total number of ratings by each user\n",
    "merged_df.loc[:, 'user_num_ratings'] = merged_df.groupby('user_id')['book_rating'].transform('count')\n",
    "\n",
    "# Create a separate dataframe for easy inspection\n",
    "user_level_features = merged_df[['user_id', 'user_avg_rating', 'user_num_ratings']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa03907",
   "metadata": {},
   "source": [
    "From our new features, we see that most users rated only one book (39,223 users), with a steep drop for 2 or 3 ratings. This highlights a severe cold-start problem, making it hard for user-based filtering to find similar users. A few users, however, rated many books, with some giving over 300 ratings, showing that highly active users could still provide useful patterns for recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a730f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many ratings each user gave\n",
    "user_counts_merged = merged_df.groupby('user_id')['book_rating'].count()\n",
    "\n",
    "# Count how many users gave exactly k ratings\n",
    "freq = user_counts_merged.value_counts().sort_index()\n",
    "\n",
    "# Limit to a sensible range (e.g., up to 30 ratings)\n",
    "max_k = 30\n",
    "freq = freq.loc[freq.index <= max_k]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = sns.barplot(x=freq.index.astype(int), y=freq.values, palette='pastel')\n",
    "plt.xlabel('Number of ratings per user')\n",
    "plt.ylabel('Number of users')\n",
    "plt.title('How many users rated exactly k books')\n",
    "\n",
    "# Dynamically adjust y-axis to leave space for numbers\n",
    "plt.ylim(0, freq.values.max() * 1.15)\n",
    "\n",
    "# Annotate bars above each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,  # center of the bar\n",
    "        height + freq.values.max() * 0.01,  # small offset above the bar\n",
    "        f\"{int(height)}\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = merged_df.groupby('user_id')['book_rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafcd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round user average ratings to nearest integer for counting\n",
    "avg_rating_rounded = merged_df.groupby('user_id')['user_avg_rating'].mean().round().astype(int)\n",
    "\n",
    "# Count how many users have each average rating\n",
    "freq_avg = avg_rating_rounded.value_counts().sort_index()\n",
    "\n",
    "# Limit to ratings 1–10\n",
    "freq_avg = freq_avg.loc[(freq_avg.index >= 1) & (freq_avg.index <= 10)]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = sns.barplot(x=freq_avg.index, y=freq_avg.values, palette='pastel')\n",
    "plt.xlabel('Average rating per user')\n",
    "plt.ylabel('Number of users')\n",
    "plt.title('Distribution of average user ratings (1-10)')\n",
    "\n",
    "# Annotate bars properly\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,  # center of the bar\n",
    "        height + height * 0.02,         # 2% of bar height as offset\n",
    "        f\"{int(height)}\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9d4f7",
   "metadata": {},
   "source": [
    "Creates a **user age group feature** by binning ages into 5 ranges (0–18, 18–25, 25–35, 35–50, 50–100) and assigning numeric group codes from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff53be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins (adjustable) and labels 1–5\n",
    "age_bins = [0, 18, 25, 35, 50, 105]  # covers ages up to 120\n",
    "age_labels = [1, 2, 3, 4, 5]         # numeric group codes\n",
    "\n",
    "# Create a new column\n",
    "merged_df['User_age_Group'] = pd.cut(\n",
    "    merged_df['age'],\n",
    "    bins=age_bins,\n",
    "    labels=age_labels,\n",
    "    right=False  # includes the left edge (e.g., 18 goes to group 2)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3f762",
   "metadata": {},
   "source": [
    "Creates **book-level features**:\n",
    "\n",
    "- **`book_avg_rating`** – average rating for each book  \n",
    "- **`book_num_ratings`** – total number of ratings per book  \n",
    "- **`book_rating_variability`** – variation in book ratings (std)  \n",
    "- **`book_popularity_score`** – popularity metric combining average rating and number of ratings  \n",
    "\n",
    "A summary dataframe is then created with these unique book-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90105c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating per book (some books have multiple isbns)\n",
    "merged_df.loc[:, 'book_avg_rating'] = merged_df.groupby('book_title')['book_rating'].transform('mean')\n",
    "\n",
    "# Number of ratings per book\n",
    "merged_df.loc[:, 'book_num_ratings'] = merged_df.groupby('book_title')['book_rating'].transform('count')\n",
    "\n",
    "# Optional: Book popularity score (weighted by number of ratings)\n",
    "merged_df.loc[:, 'book_popularity_score'] = merged_df['book_avg_rating'] * np.log1p(merged_df['book_num_ratings'])\n",
    "\n",
    "# Create a distinct book-level summary DataFrame\n",
    "book_level_features = merged_df[['book_title', 'book_avg_rating', 'book_num_ratings', 'book_popularity_score']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ef7aa",
   "metadata": {},
   "source": [
    "Looking at how many ratings each book received, most books were rated only once (88,137 books), with the number of books dropping sharply for 2 or 3 ratings. This shows that the dataset is sparse, making it harder for item-based collaborative filtering to find similar books. A few books, however, received hundreds of ratings, with some exceeding 500, indicating that popular books can still provide strong signals for recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897de7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many ratings each book received\n",
    "book_counts = merged_df.groupby('book_title')['book_rating'].count()\n",
    "freq_books = book_counts.value_counts().sort_index()  # index = number of ratings, value = number of books\n",
    "\n",
    "# Optionally limit to a sensible range\n",
    "max_k = 20\n",
    "freq_books = freq_books.loc[freq_books.index <= max_k]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "ax = sns.barplot(x=freq_books.index.astype(int), y=freq_books.values, palette='pastel')\n",
    "plt.xlabel('Number of ratings per book')\n",
    "plt.ylabel('Number of books')\n",
    "plt.title('How many books received exactly k ratings')\n",
    "\n",
    "# Annotate bars\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,  # center of the bar\n",
    "        height + height * 0.02,         # 2% of bar height offset\n",
    "        f\"{int(height)}\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_counts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round book average ratings to nearest integer\n",
    "avg_rating_books = merged_df.groupby('book_title')['book_avg_rating'].mean().round().astype(int)\n",
    "\n",
    "# Count how many books have each average rating\n",
    "freq_books_avg = avg_rating_books.value_counts().sort_index()\n",
    "\n",
    "# Limit to ratings 1–10\n",
    "freq_books_avg = freq_books_avg.loc[(freq_books_avg.index >= 1) & (freq_books_avg.index <= 10)]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "colors = sns.color_palette('pastel', n_colors=len(freq_books_avg))\n",
    "ax = sns.barplot(x=freq_books_avg.index, y=freq_books_avg.values, palette=colors)\n",
    "plt.xlabel('Average rating per book')\n",
    "plt.ylabel('Number of books')\n",
    "plt.title('Distribution of average book ratings (1-10)')\n",
    "\n",
    "# Annotate bars properly\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,  # center of the bar\n",
    "        height + height * 0.02,         # 2% of bar height offset\n",
    "        f\"{int(height)}\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56468db5",
   "metadata": {},
   "source": [
    "This section creates additional features at different levels:\n",
    "\n",
    "- **Author-level:** `author_avg_rating` – the average rating of all books by each author.  \n",
    "- **Publisher-level:** `publisher_avg_rating` – the average rating of all books by each publisher.  \n",
    "- **Book age:** `book_age` – the age of each book calculated as the difference between the current year (2025) and its year of publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb80597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AUTHOR-LEVEL FEATURE ---\n",
    "merged_df.loc[:, 'author_avg_rating'] = merged_df.groupby('book_author')['book_rating'].transform('mean')\n",
    "\n",
    "# --- publisher-LEVEL FEATURE ---\n",
    "merged_df.loc[:, 'publisher_avg_rating'] = merged_df.groupby('publisher')['book_rating'].transform('mean')\n",
    "\n",
    "# --- BOOK age FEATURE ---\n",
    "current_year = 2025\n",
    "merged_df.loc[:, 'book_age'] = current_year - merged_df['year_of_publication']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784afe6",
   "metadata": {},
   "source": [
    "This section creates **location-based features**:\n",
    "\n",
    "- **`country_avg_rating`** – average rating per country.  \n",
    "- **`region_avg_rating`** – average rating per region.  \n",
    "- **`city_avg_rating`** – average rating per city.  \n",
    "- **`user_country_rating_bias`** – difference between a user’s average rating and their country’s average, capturing personal bias.  \n",
    "- **`book_country_avg_rating`** – average rating of each book within each country, reflecting local popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcdfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- location-BASED FEATURES ---\n",
    "\n",
    "# Country-level average rating\n",
    "merged_df.loc[:, 'country_avg_rating'] = merged_df.groupby('country_clean')['book_rating'].transform('mean')\n",
    "\n",
    "# region-level average rating (if region exists)\n",
    "merged_df.loc[:, 'region_avg_rating'] = merged_df.groupby('region')['book_rating'].transform('mean')\n",
    "\n",
    "# City-level average rating (optional, more granular)\n",
    "merged_df.loc[:, 'city_avg_rating'] = merged_df.groupby('city_clean')['book_rating'].transform('mean')\n",
    "\n",
    "# User’s bias compared to their country's average\n",
    "merged_df.loc[:, 'user_country_rating_bias'] = merged_df['user_avg_rating'] - merged_df['country_avg_rating']\n",
    "\n",
    "# Average rating for each Book within each Country\n",
    "merged_df.loc[:, 'book_country_avg_rating'] = merged_df.groupby(['book_title', 'country_clean'])['book_rating'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b778985",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388a555",
   "metadata": {},
   "source": [
    "# **5.2. Plots and More Insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for a single top-rated book\n",
    "book_name = filtered_books_500.iloc[0]['book_title']  # top book\n",
    "book_data = merged_df[merged_df['book_title'] == book_name].copy()\n",
    "\n",
    "# Add small jitter to age to avoid overlapping points\n",
    "jitter_strength = 0.5\n",
    "book_data['age_jitter'] = book_data['age'] + np.random.uniform(-jitter_strength, jitter_strength, size=len(book_data))\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter with jitter\n",
    "sns.scatterplot(\n",
    "    data=book_data,\n",
    "    x='age_jitter',\n",
    "    y='book_rating',\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Regression line\n",
    "sns.regplot(\n",
    "    data=book_data,\n",
    "    x='age',\n",
    "    y='book_rating',\n",
    "    scatter=False,\n",
    "    color='red'\n",
    ")\n",
    "\n",
    "plt.title(f'User Age vs Rating for \"{book_name}\"', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('User Age')\n",
    "plt.ylabel('Book Rating')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=merged_df.drop_duplicates('isbn'),\n",
    "                x='book_popularity_score', y='book_avg_rating', alpha=0.5)\n",
    "sns.regplot(data=merged_df.drop_duplicates('isbn'),\n",
    "            x='book_popularity_score', y='book_avg_rating', scatter=False, color='red')\n",
    "plt.title('Book Popularity Score vs Average Rating')\n",
    "plt.xlabel('Book Popularity Score')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16016ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_age = (\n",
    "    merged_df.groupby(['book_author', 'User_age_Group'])['book_rating']\n",
    "    .mean().reset_index()\n",
    ")\n",
    "heat = author_age.pivot(index='book_author', columns='User_age_Group', values='book_rating')\n",
    "\n",
    "# Optional: limit to top authors by number of ratings\n",
    "top_authors = merged_df.groupby('book_author')['book_rating'].count().nlargest(40).index\n",
    "heat = heat.reindex(top_authors)\n",
    "\n",
    "# sort by group 1 if present, otherwise by row mean\n",
    "sort_col = 1\n",
    "if sort_col in heat.columns:\n",
    "    heat = heat.sort_values(by=sort_col, ascending=False)\n",
    "else:\n",
    "    heat = heat.assign(_mean=heat.mean(axis=1)).sort_values('_mean', ascending=False).drop(columns=['_mean'])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(heat, annot=True, fmt=\".2f\", linewidths=.4, cbar_kws={'label':'Avg rating'})\n",
    "plt.title('Author vs User Age Group — Average Rating')\n",
    "plt.xlabel('User Age Group')\n",
    "plt.ylabel('Author')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_age = (\n",
    "    merged_df.groupby(['publisher', 'User_age_Group'])['book_rating']\n",
    "    .mean().reset_index()\n",
    ")\n",
    "heat = pub_age.pivot(index='publisher', columns='User_age_Group', values='book_rating')\n",
    "\n",
    "# limit to most prolific publishers\n",
    "top_pubs = merged_df.groupby('publisher')['book_rating'].count().nlargest(30).index\n",
    "heat = heat.reindex(top_pubs)\n",
    "\n",
    "# sort safely\n",
    "if 1 in heat.columns:\n",
    "    heat = heat.sort_values(by=1, ascending=False)\n",
    "else:\n",
    "    heat = heat.assign(_mean=heat.mean(axis=1)).sort_values('_mean', ascending=False).drop(columns=['_mean'])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(heat, annot=True, fmt=\".2f\", linewidths=.4, cbar_kws={'label':'Avg rating'})\n",
    "plt.title('Publisher vs User Age Group — Average Rating')\n",
    "plt.xlabel('User Age Group')\n",
    "plt.ylabel('Publisher')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_books (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
