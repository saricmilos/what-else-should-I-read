% Author: Milos A. Saric, copyright 2025. https://saricmilos.com/

\title{Book Recommendation System --- Collaborative \& Content-Based Filtering Approaches}
\date{\today}
\author{Milos Saric \\[1mm] {\small \socials}}
\maketitle



% --------------- ABSTRACT
\begin{abstract}
This report investigates the Kaggle Book Recommendation dataset to develop intelligent book recommendation systems using both
collaborative filtering and content-based methods. A key application of machine learning is generating personalized recommendations
for users, aiming to boost revenue, engagement, or other performance metrics. As a result, understanding how these recommendation
algorithms work is an essential skill for any Machine Learning Engineer.
Recommender systems have become a vital component of online platforms such
as YouTube, Amazon, and Netflix, helping users discover content tailored to their preferences. By analyzing user behavior and
item attributes, these systems predict user interests, enhancing user experience while driving engagement and revenue. 
This study highlights the implementation, advantages, and practical significance of recommendation algorithms in modern
digital platforms.
\end{abstract}

\rule{\linewidth}{0.5pt}

\section{Introduction and Problem Definition}

Recommender systems aim to predict user preferences and suggest items they are most likely to enjoy. 
This project focuses on developing a \textbf{book recommendation system} using the Kaggle \textbf{Book-Crossing Dataset},
leveraging both \textbf{collaborative}, \textbf{content-based approaches} and \textbf{hybrid filtering approaches} to deliver personalized
book suggestions.
The goal is to design a system that accurately predicts and recommends books a user is likely to enjoy based on past interactions,
ratings, and preferences. A clear understanding of this problem ensures that all subsequent analysis and modeling efforts align
with the primary objective.

The main purpose of a recommender system is to boost product sales. Businesses use these systems to maximize their profits by
suggesting products that customers are likely to be interested in. By highlighting relevant and appealing items, recommender
systems help users discover products they might otherwise miss — ultimately driving higher sales and greater profits for the company.

\begin{center}
\begin{tikzpicture}[node distance=2cm,
    rec/.style={rectangle, draw=black, fill=#1!40, rounded corners, minimum width=3.5cm, minimum height=1cm, align=center, font=\small},
    arrow/.style={-{Stealth}, thick},
    every node/.style={font=\small}
]

    % Nodes with colors
    \node[rec=orange] (RS) {Recommender Systems};
    \node[rec=cyan] (CF) [below left of=RS, xshift=-3cm] {Collaborative Filtering};
    \node[rec=green] (CB) [below of=RS] {Content-Based Filtering};
    \node[rec=purple] (Hybrid) [below right of=RS, xshift=3cm] {Hybrid Filtering};
    \node[rec=blue] (User) [below left of=CF, xshift=-1.5cm, yshift=-0.5cm] {User-Based};
    \node[rec=red] (Item) [below right of=CF, xshift=1.5cm, yshift=-0.5cm] {Item-Based};

    % Arrows
    \draw[arrow] (RS) -- (CF);
    \draw[arrow] (RS) -- (CB);
    \draw[arrow] (RS) -- (Hybrid);
    \draw[arrow] (CF) -- (User);
    \draw[arrow] (CF) -- (Item);
\end{tikzpicture}
\end{center}

\subsection{Scope}
The analysis focuses exclusively on the Kaggle dataset, which includes:  
\begin{itemize}
    \item \textbf{Users:} demographic and identification data.  
    \item \textbf{Books:} metadata such as title, author, year, publisher, and cover images.  
    \item \textbf{Ratings:} explicit ratings (1--10) and implicit feedback (0 for interactions without ratings).  
\end{itemize}
Predictions are restricted to this dataset without using external sources unless explicitly integrated in advanced phases.

\subsection{Stakeholders}
\begin{itemize}
    \item \textbf{Readers / Users:} receive personalized book recommendations.  
    \item \textbf{Publishers / Authors:} gain insights into reader interests for better targeting.  
    \item \textbf{Data Scientists / ML Practitioners:} test and optimize recommendation algorithms.  
    \item \textbf{Platform Developers / Businesses:} improve user engagement, retention, and revenue through effective recommendations.
\end{itemize}


\section{Neighborhood-Based Collaborative Filtering}\label{sec:collaborative_filter}

Collaborative filtering creates a model based on a user's past actions such as items they have purchased, selected, or
rated, as well as the behavior of other users with similar preferences. This model is then used to recommend items or predict 
ratings for items that the user is likely to be interested in.
Collaborative filtering is a recommendation method that predicts a users preferences based on the choices of other users
with similar tastes. It relies on both user and item information, often organized in a user-item matrix.

This approach is widely used in industry across platforms like YouTube, Netflix and Amazon. 
For example, if users with similar past actions/tastes buy a product on Amazon, the system can suggest that same product to you.
YouTube leverages this technique to recommend videos, Amazon uses it to suggest books and products, and Netflix relies heavily on
collaborative filtering to personalize movie recommendations.

\begin{center}
\begin{tikzpicture}[
    user/.style={rectangle, draw=black, fill=cyan!25, rounded corners, minimum width=2.6cm, minimum height=0.9cm, align=center, font=\small},
    item/.style={rectangle, draw=black, fill=green!20, rounded corners, minimum width=2.6cm, minimum height=0.9cm, align=center, font=\small},
    like/.style={-{Stealth}, thick},
    recommend/.style={-{Stealth}, thick, dashed, red},
    similar/.style={<->, very thick, purple!70},
    every node/.style={font=\small}
]

    % Users
    \node[user] (User1) {User 1};
    \node[user] (User2) [right=6.0cm of User1] {User 2};

    % Similarity label and arrow between users
    \draw[similar] (User1) -- node[above=3pt]{\footnotesize similar} (User2);

    % Books for User1 (spaced out horizontally)
    \node[item] (Book1) [below=3.0cm of User1, xshift=-2.0cm] {1984};
    \node[item] (Book2) [below=3.0cm of User1, xshift= 2.0cm] {Steve Jobs};

    % Books for User2 (spaced out horizontally, wider spacing)
    \node[item] (Book3) [below=3.0cm of User2, xshift=-3.0cm] {1984};
    \node[item] (Book4) [below=3.0cm of User2, xshift= 0.0cm] {Steve Jobs};
    \node[item] (Book5) [below=3.0cm of User2, xshift= 3.0cm] {Stiff};

    % Arrows from User1 to books (solid)
    \draw[like] (User1.south) -- (Book1.north);
    \draw[like] (User1.south) -- (Book2.north);

    % Arrows from User2 to books (solid)
    \draw[like] (User2.south) -- (Book3.north);
    \draw[like] (User2.south) -- (Book4.north);
    \draw[like] (User2.south) -- (Book5.north);

    % Recommendation arrow from Book5 to User1 with label above the line
    \draw[recommend] (Book5.north) to[out=90,in=40] node[midway, above]{\footnotesize recommend} (User1.south);

    % Legend (top-left)
    \node[draw=none, fill=none, above=1.1cm of User1, align=left] (legend) {
      \begin{tabular}{@{}ll@{}}
        \raisebox{0.5ex}{\tikz{\draw[like] (0,0)--(0.6,0);}} & like \\
        \raisebox{0.5ex}{\tikz{\draw[recommend] (0,0)--(0.6,0);}} & recommend \\
        \raisebox{0.5ex}{\tikz{\draw[similar] (0,0)--(0.6,0);}} & similar
      \end{tabular}
    };

\end{tikzpicture}
\end{center}

\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[every node/.style={transform shape},
    user/.style={rectangle, draw=black, fill=cyan!25, rounded corners, minimum width=2.2cm, minimum height=0.9cm, align=center, font=\small},
    item/.style={rectangle, draw=black, fill=green!18, rounded corners, minimum width=2.2cm, minimum height=0.9cm, align=center, font=\small},
    like/.style={-{Stealth}, thick},
    recommend/.style={-{Stealth}, thick, dashed, red},
    similar/.style={<->, very thick, purple!60}
]

%%%%% Use symmetric xshifts so left and right diagrams are identical in size
\begin{scope}[xshift=-6.5cm]  % left diagram (negative shift)
    % Users (vertical)
    \node[user] (A) {User A};
    \node[user, below=0.9cm of A] (B) {User B};
    \node[user, below=0.9cm of B] (C) {User C};

    % Items (to the right of users), spaced to avoid overlap
    \node[item, right=3.6cm of A] (IA) {Book A};
    \node[item, below=1.1cm of IA] (IB) {Book B};
    \node[item, below=1.1cm of IB] (IC) {Book C};
    \node[item, below=1.1cm of IC] (ID) {Book D};

    % Similarity label & arrow (left side connecting A and C)
    \draw[-{Stealth}, thick] ($(A.west)+(-0.6,0)$) -- ($(C.west)+(-0.6,0)$) node[midway,left]{\footnotesize similar};

    % Like arrows
    \draw[like] (A) -- (IA);
    \draw[like] (A) -- (IC);
    \draw[like] (B) -- (IB);
    \draw[like] (C) -- (IA);
    \draw[like] (C) -- (IC);
    \draw[like] (C) -- (ID);

    % Recommend arrow (dashed red) from ID to A with label above
    \draw[recommend] (ID.north) to[out=85,in=0] node[midway, above]{\footnotesize recommend} (A.south);

    % Legend
    \node[draw=none, fill=none, above=0.9cm of A, align=left] (legendL) {
      \begin{tabular}{@{}ll@{}}
        \raisebox{0.5ex}{\tikz{\draw[like] (0,0)--(0.6,0);}} & like \\
        \raisebox{0.5ex}{\tikz{\draw[recommend] (0,0)--(0.6,0);}} & recommend \\
        \raisebox{0.5ex}{\tikz{\draw[similar] (0,0)--(0.6,0);}} & similar
      \end{tabular}
    };

    % Caption
    \node[below=1.2cm of C] {\textbf{User-Based CF}};
\end{scope}

\begin{scope}[xshift=+6.5cm]  % right diagram (positive shift)
    % Users (vertical)
    \node[user] (U1) {User A};
    \node[user, below=0.9cm of U1] (U2) {User B};
    \node[user, below=0.9cm of U2] (U3) {User C};

    % Items (to the right of users) — same spacing as left
    \node[item, right=3.6cm of U1] (IA2) {Book A};
    \node[item, below=1.1cm of IA2] (IB2) {Book B};
    \node[item, below=1.1cm of IB2] (IC2) {Book C};
    \node[item, below=1.1cm of IC2] (ID2) {Book D};

    % Like arrows (solid)
    \draw[like] (U1) -- (IA2);
    \draw[like] (U1) -- (IC2);
    \draw[like] (U1) -- (ID2);
    \draw[like] (U2) -- (IA2);
    \draw[like] (U2) -- (ID2);
    \draw[like] (U3) -- (IA2);

    % Recommend arrow from ID2 to U3 with label above the line
    \draw[recommend] (ID2.north) to[out=85,in=20] node[midway, above]{\footnotesize recommend} (U3.south);

    % Similarity between items (right side) — arrow & label
    \draw[-{Stealth}, thick] (IA2.east) -- ++(0.9,-2.8) node[midway,right]{\footnotesize similar};
    \draw[like] (ID2.east) -- ++(0.9,2.8);

    % Legend
    \node[draw=none, fill=none, above=0.9cm of U1, align=left] (legendR) {
      \begin{tabular}{@{}ll@{}}
        \raisebox{0.5ex}{\tikz{\draw[like] (0,0)--(0.6,0);}} & like \\
        \raisebox{0.5ex}{\tikz{\draw[recommend] (0,0)--(0.6,0);}} & recommend
      \end{tabular}
    };

    % Caption
    \node[below=1.2cm of U3] {\textbf{Item-Based CF}};
\end{scope}

\end{tikzpicture}%
} % end resizebox
\end{center}

Collaborative filtering relies heavily on user data. As users continue to browse, purchase, and rate products, their preferences evolve over time. Because of this, the underlying recommendation model must be regularly updated and refined to stay accurate and relevant. In practice, this means continuously integrating new data and adjusting the system to reflect changing user behavior and market trends.

The two main sources of data for collaborative filtering are:

\begin{itemize}
    \item \textbf{Explicit data:} Information that users actively provide, such as ratings, reviews, or responses to surveys and questionnaires.
    \item \textbf{Implicit data:} Information inferred from user behavior, including browsing history, clicks, time spent on content, likes, shares, and other engagement signals.
\end{itemize}

Both types of data are essential for understanding user preferences. Explicit ratings capture direct feedback, while implicit data reveals natural behavioral patterns that help improve recommendation accuracy over time.

\subsection{User-Based Collaborative Filtering}
User-based collaborative filtering recommends items to a user by identifying other users with similar preferences. 
If a similar user liked an item, it can be suggested. 
In our user based book recommendation system, we begin by constructing a matrix with users as rows and books (identified by ISBN) 
as columns let's call this the user-book matrix.

Each entry in the matrix represents either a rating from 1 to 10, indicating that the user has read and rated the book,
or a zero if the user has not interacted with that book. For example, if user J has rated book K, the corresponding cell
contains that rating. If user J has not read or rated book K, the cell is simply zero.

User-based collaborative filtering recommends items to a user by identifying other users with similar preferences.
If a similar user liked an item, it can be suggested. 

Similarity between users can be measured using metrics such as 
\hyperref[sec:cosine]{\textbf{Cosine similarity}}, 
\hyperref[sec:pearson]{\textbf{Pearson correlation}}, or 
\hyperref[sec:euclidean]{\textbf{Euclidean distance}}. 
Read more about similarity at Section~\ref{sec:similarity}.

\subsubsection{Algorithmic Workflow}
\begin{enumerate}
    \item Represent each user as a vector in item space:

        In user-based collaborative filtering, we start from a set of observed user-item ratings
        \begin{equation}
        R = \{(u, i, r_{ui}) \mid u \in U,\, i \in I \}
        \end{equation}
        where $U$ is the set of users, $I$ is the set of items (books), and $r_{ui}$ is the rating given by user $u$ to item $i$.

        We represent this data as a \textbf{user-item rating matrix} $M \in \mathbb{R}^{|U| \times |I|}$:
        \begin{equation}
        M_{ui} =
        \begin{cases}
        r_{ui}, & \text{if user } u \text{ rated item } i \\
        0, & \text{otherwise.}
        \end{cases}
        \end{equation}

    Each \textbf{row} of this matrix corresponds to a \emph{user vector}:
    \begin{equation}
    \mathbf{u}_k = [\, r_{k1}, r_{k2}, \ldots, r_{k|I|} \,] \in \mathbb{R}^{|I|}
    \end{equation}
    where \(r_{kj}\) is the rating of user \(k\) for item \(j\), and \(I\) is the number of items.
    Thus, the moment we pivot our data from a triplet $(u, i, r_{ui})$ format into a matrix form,
    we have implicitly constructed a high-dimensional vector representation for every user.
    
    \item Compute user-user similarity, e.g., using \textbf{cosine similarity}:
    \begin{equation}
    \text{sim}(u, v) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \, \|\mathbf{v}\|}
    \end{equation}
    where \(\mathbf{u} \cdot \mathbf{v}\) is the dot product, and \(\|\mathbf{u}\|\) is the Euclidean norm:
    \begin{equation}
    \|\mathbf{u}\| = \sqrt{\sum_{j=1}^{M} r_{uj}^2}
    \end{equation}

    \begin{itemize}
    \item Each user is a vector in \(M\)-dimensional space, where each axis represents an item.
    \item The length of the vector corresponds to the magnitude of their ratings.
    \item Cosine similarity measures the \textbf{angle between user vectors}, i.e., the similarity in rating patterns.
    \item Example:

\[
\text{User A} = [5, 3, 0], \quad
\text{User B} = [4, 2, 0]
\]

Even though User B gives slightly lower ratings, the \textit{pattern is similar}, resulting in a high cosine similarity.
\end{itemize}



\tdplotsetmaincoords{60}{120}
\begin{center}
\begin{tikzpicture}[tdplot_main_coords, scale=5]

% Draw axes
\draw[->] (0,0,0) -- (1.2,0,0) node[anchor=north east]{$\text{Item 1}$};
\draw[->] (0,0,0) -- (0,1.2,0) node[anchor=north west]{$\text{Item 2}$};
\draw[->] (0,0,0) -- (0,0,1.2) node[anchor=south]{$\text{Item 3}$};

% Draw user vectors
\draw[->, thick, blue] (0,0,0) -- (1,0.6,0) node[midway, above] {User A};
\draw[->, thick, red] (0,0,0) -- (0.8,0.4,0) node[midway, above right] {User B};
\draw[->, thick, green] (0,0,0) -- (0.2,1,0.6) node[midway, right] {User C};

% Optionally draw angle between A and B
\draw[dashed] (0,0,0) -- (1,0.6,0);
\draw[dashed] (0,0,0) -- (0.8,0.4,0);

\end{tikzpicture}
\end{center}

Cosine similarity compares rating patterns, not absolute rating levels.
Length of the vector = total “strength” of user ratings, but cosine similarity is normalized so that only direction matters.
    
    \item Predict ratings for a user \(u\) on item \(i\) using top-K similar users \(N(u)\):

    There are two related but different formulas commonly used for user-based collaborative filtering (CF). 

    The correct and most widely used formula for predicting a user's rating is:

    \begin{equation}
        \hat{r}_{ui} = \bar{r}_u + 
        \frac{
            \sum_{v \in N(u)} \text{sim}(u,v) \cdot (r_{vi} - \bar{r}_v)
        }{
            \sum_{v \in N(u)} |\text{sim}(u,v)|
            }
    \end{equation}

    \begin{itemize}
        \item $\hat{r}_{ui}$ is the \textit{predicted rating} of user $u$ for item $i$.
        \item $\bar{r}_u$ represents the \textit{average rating} of user $u$ (the user’s rating bias).
        \item $N(u)$ denotes the set of \textit{nearest neighbors} (similar users to $u$) who have rated item $i$.
        \item $r_{vi}$ is the \textit{rating given by neighbor} $v$ to item $i$.
        \item $\bar{r}_v$ is the \textit{average rating} of user $v$.
        \item $\text{sim}(u,v)$ measures the \textit{similarity} between users $u$ and $v$, often computed using cosine similarity, Pearson correlation, or other metrics.
    \end{itemize}

    A simpler, less accurate version of the formula is sometimes used:

    \begin{equation}
        \hat{r}_{ui} = 
    \frac{
        \sum_{v \in N(u)} s(u,v) \cdot r_{vi}
        }{
        \sum_{v \in N(u)} |s(u,v)|
        }
    \end{equation}

    \begin{itemize}
        \item This version ignores user rating biases ($\bar{r}_u$ and $\bar{r}_v$).
        \item It assumes all users rate on the same scale, which is often not true in practice.
        \item Although simpler, it generally performs worse in predicting ratings accurately.
    \end{itemize}

In summary, the mean-centered formula is preferred for more accurate and bias-adjusted predictions, while the simpler one can be used as a baseline or when user averages are unavailable.

\item Evaluation of Collaborative Filtering Predictions:

Evaluating the performance of a collaborative filtering (CF) model involves 
measuring how accurately the system predicts user ratings and how effective 
its recommendations are. Two major evaluation perspectives are commonly used:
prediction accuracy and recommendation quality.

    \item \textbf{Evaluation Procedure}

    A standard workflow:

    \begin{enumerate}
        \item Split the dataset into training and test sets (e.g., 80/20 split).
        \item Build the CF model using the training set.
        \item Predict ratings $\hat{r}_{ui}$ for test set user--item pairs.
        \item Compare predicted and actual ratings using MAE, RMSE, etc.
        \item Optionally, evaluate top-$N$ recommendations using Precision@N and Recall@N.
    \end{enumerate}

\end{enumerate}

\subsubsection{Summary}

\begin{itemize}
    \item Each user is a vector in an $|I|$-dimensional space (items are dimensions).
    \item Ratings are vector components (coordinate values).
    \item Similarities are computed between users' vectors.
    \item Predictions are weighted averages of neighbors' ratings.
\end{itemize}

\subsection{Item-Based Collaborative Filtering}

Item-based collaborative filtering recommends items to a user by analyzing similarities between items rather than users, based on user interactions.  
The intuition is: if a user liked a particular item, they are likely to enjoy other items that received similar ratings from other users.  
For example, in a book recommendation system, if two books are rated similarly by many users, they are considered similar, and a user who liked one might like the other.

\paragraph{Matrix Representation}
The cosine similarity function itself does not know whether we are computing
user-based or item-based similarity. The distinction depends solely on the orientation
of the rating matrix $M$.
We have two perspectives:
\begin{enumerate}
    \item \textbf{User-Based Filtering:}
    Each user is represented as a vector in the \emph{item space}. This yields a user--user similarity matrix of shape $|U| \times |U|$.
    \item \textbf{Item-Based Filtering:}
    Each item is represented as a vector in the \emph{user space}. This yields an item--item similarity matrix of shape $|I| \times |I|$.
\end{enumerate}   
In practice, this difference is implemented by transposing the matrix before applying
cosine similarity:
\[
\text{user-based: } \text{cosine\_similarity}(M), \quad
\text{item-based: } \text{cosine\_similarity}(M^T).
\] 
Each item becomes a vector in \emph{user space}, with dimensions corresponding to users:

\begin{equation}
\mathbf{i}_j = [r_{1j}, r_{2j}, r_{3j}, \dots, r_{Nj}] \in \mathbb{R}^{|U|}
\end{equation}

where \(r_{uj}\) is the rating given by user \(u\) to item \(j\), and \(N = |U|\) is the number of users.  
The transposition means that the same rating matrix \(M\) can be used, but similarity computations are performed along the columns instead of rows.

\paragraph{Item Similarity}

Similar to user-based CF, we compute the similarity between item vectors using metrics like cosine similarity:

\begin{equation}
\text{sim}(i, j) = \frac{\mathbf{i}_i \cdot \mathbf{i}_j}{\|\mathbf{i}_i\| \, \|\mathbf{i}_j\|}
\end{equation}

This produces an \(|I| \times |I|\) item-item similarity matrix. The similarity values are then used to predict a user's rating for a target item based on the ratings of similar items they have already rated.

\paragraph{Prediction Formula}

The predicted rating of user \(u\) for item \(i\) is computed as:

\begin{equation}
\hat{r}_{ui} = \frac{\sum_{j \in N(i)} s(i,j) \cdot r_{uj}}{\sum_{j \in N(i)} |s(i,j)|}
\end{equation}

where:

\begin{itemize}
    \item \(N(i)\) is the set of top-K items similar to \(i\) that user \(u\) has rated.
    \item \(s(i,j)\) is the similarity between items \(i\) and \(j\).
    \item \(r_{uj}\) is the rating of user \(u\) for item \(j\).
\end{itemize}

This formula is analogous to the user-based CF prediction, but here the weights come from item-item similarity rather than user-user similarity.

\paragraph{Algorithmic Workflow}

\begin{enumerate}
    \item Represent each item as a vector in user space (\(|U|\)-dimensional).
    \item Compute the item-item similarity matrix using cosine similarity.
    \item For each user and target item, identify the set of similar items the user has rated (\(N(i)\)).
    \item Predict the rating using a weighted sum of the user’s ratings on similar items.
    \item Optionally, generate top-N recommendations based on predicted ratings.
\end{enumerate}

\paragraph{Intuition and Advantages}

\begin{itemize}
    \item Each item is a vector in user space, analogous to users being vectors in item space for user-based CF.
    \item Cosine similarity measures which items are rated similarly across users.
    \item IBCF often performs better in sparse datasets, as items tend to have more ratings than individual users, making similarity computations more robust.
    \item Reduces computational cost when the number of items is smaller than the number of users.
\end{itemize}

\begin{center}
\tdplotsetmaincoords{60}{120}
\begin{tikzpicture}[tdplot_main_coords, scale=5]

% Draw axes
\draw[->] (0,0,0) -- (1.2,0,0) node[anchor=north east]{$\text{User 1}$};
\draw[->] (0,0,0) -- (0,1.2,0) node[anchor=north west]{$\text{User 2}$};
\draw[->] (0,0,0) -- (0,0,1.2) node[anchor=south]{$\text{User 3}$};

% Draw item vectors
\draw[->, thick, blue] (0,0,0) -- (1,0.6,0) node[midway, above] {Item A};
\draw[->, thick, red] (0,0,0) -- (0.8,0.4,0) node[midway, above right] {Item B};
\draw[->, thick, green] (0,0,0) -- (0.2,1,0.6) node[midway, right] {Item C};

% Optionally dashed lines for visual reference
\draw[dashed] (0,0,0) -- (1,0.6,0);
\draw[dashed] (0,0,0) -- (0.8,0.4,0);

\end{tikzpicture}
\end{center}

\section{Content Based Filtering}

Content-Based Filtering recommends items that are \textit{similar to those a user has previously liked}.  
It relies on the features or attributes of the items, such as movie genre, product category, author, or keywords.  
The system first builds a user profile based on the characteristics of items the user interacted with, then suggests other items with similar properties.  
It generates recommendations by analyzing the features of items and matching them to a user's preferences. Essentially, the system compares a \textbf{user profile} with an \textbf{item profile} to predict which items the user is likely to engage with.
Content-Based Recommender Systems often build a personalized model for each user. The process begins by collecting the features of items the user has interacted with which forms the user profile. These items serve as a training set for a user-specific classifier or regression model. 

In the model, the item features act as independent variables, while the user's past behavior-such as ratings, likes, 
or purchases serve as the dependent variable. Once trained, the model predicts the user's likely response to new items, 
allowing the system to recommend items that align with the user's preferences.
The \textbf{item profile} captures the characteristics of an item, including structured features or descriptive metadata. For example, a streaming service may represent movies using attributes such as genre, director, release year, or cast.  

The \textbf{user profile} reflects the individual's preferences and past behavior. It is typically built from items the user has previously interacted with, including ratings, likes, dislikes, searches, or other engagement data.  
In short, this approach focuses on \textit{what the user likes}, rather than what other users like.

\subsection{Item Representations in Content-Based Filtering}

In content-based filtering, items and users are often represented as vectors in a multi-dimensional feature space. 
Each item's feature such as genre, author, or other metadata define its coordinates. 
Boolean values (1 for presence, 0 for absence) are commonly used to indicate whether an item possesses a particular feature.

\textbf{Example:} A few novels represented by three genres (Biography, Fantasy, Thriller):

\begin{table}[H]
\centering
\caption{Boolean Feature Representation of Novels}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Novel} & \textbf{Biography} & \textbf{Fantasy} & \textbf{Thriller} \\
\hline
Steve Jobs & 0 & 1 & 1 \\
1984 & 0 & 1 & 0 \\
The Hobbit & 1 & 0 & 1 \\
The Da Vinci Code & 1 & 0 & 1 \\
\hline
\end{tabular}
\end{table}

In this 3D vector space, each dimension corresponds to a feature (Adventure, Bildungsroman, Children). A novel’s coordinates are determined by its Boolean values. For example:

\tdplotsetmaincoords{70}{120}

\begin{center}
\begin{tikzpicture}[scale=3,tdplot_main_coords]
    % Axes
    \draw[->] (0,0,0) -- (1.5,0,0) node[anchor=north east] {Thriller (X)};
    \draw[->] (0,0,0) -- (0,1.5,0) node[anchor=south west] {Fantasy (Y)};
    \draw[->] (0,0,0) -- (0,0,1.5) node[anchor=south] {Biography (Z)};
    
    % Coordinates
    \coordinate (SJ) at (0,1,1);
    \coordinate (N1984) at (0,1,0);
    \coordinate (HOBBIT) at (1,0,1);
    \coordinate (DAVINCI) at (1,0,1);
    
    % Draw vectors (lines from origin)
    \draw[->, thick, red] (0,0,0) -- (SJ);
    \draw[->, thick, blue] (0,0,0) -- (N1984);
    \draw[->, thick, green!70!black] (0,0,0) -- (HOBBIT);
    \draw[->, thick, orange!90!black] (0,0,0) -- (DAVINCI);
    
    % Points and labels
    \filldraw[red] (SJ) circle (0.03) node[above right] {\textit{Steve Jobs}};
    \filldraw[blue] (N1984) circle (0.03) node[below right] {\textit{1984}};
    \filldraw[green!70!black] (HOBBIT) circle (0.03) node[above left] {\textit{The Hobbit}};
    \filldraw[orange!90!black] (DAVINCI) circle (0.03) node[below left] {\textit{The Da Vinci Code}};
\end{tikzpicture}
\end{center}


The closer two novels are in this space, the more similar they are according to the selected features. For instance,
 \textit{The Hobbit} and \textit{The Da Vinci Code} overlap completely, while \textit{Steve Jobs} is closer to
  \textit{1984} than to the thriller novels.  Since The Hobbit and The Da Vinci Code are very similar in the feature space,
   a user who has previously purchased The Hobbit is likely to be recommended The Da Vinci Code, as it closely matches their interests.

\textbf{Visualization:} We can plot these items in a 3D Cartesian coordinate system, with each axis representing one genre. Novel vectors are points in this space, and proximity reflects similarity. Adding more features (e.g., Fantasy, Gothic) increases the dimensionality, adjusting item positions accordingly.

In content-based filtering, each user is represented by a vector that summarizes their preferences. This user vector, denoted $\mathbf{u}_x$, is typically computed as a weighted average of the vectors of the books the user has liked or rated highly:

\begin{equation}
\mathbf{u}_x = \frac{1}{N_x} \sum_{i \in I_x} w_{xi} \cdot \mathbf{v}_i
\end{equation}

where $I_x$ is the set of books user $x$ has interacted with, $w_{xi}$ is the weight associated with book $i$ for this user (such as a normalized rating or binary like/dislike), $\mathbf{v}_i$ is the feature vector of book $i$, and $N_x$ is a normalization factor.

Once the user vector is constructed, the next step is to compute the similarity between this user vector and the vector of every book in the catalog. The most common similarity measure is cosine similarity, defined as

\begin{equation}
\text{sim}(\mathbf{u}_x, \mathbf{v}_i) = \frac{\mathbf{u}_x \cdot \mathbf{v}_i}{\|\mathbf{u}_x\| \, \|\mathbf{v}_i\|}
\end{equation}

High similarity indicates that the book aligns closely with the user's preferences, making it a strong candidate for recommendation. 
Low similarity suggests the book is less likely to match the user's interests. After computing the similarity for all books,
they are sorted by score, and the top-$K$ books are recommended to the user, excluding those they have already rated. 
This process allows the system to suggest books that are most likely to match the user's tastes based on the features encoded
in the vectors.

\begin{figure}[H]
\centering
\scalebox{0.85}{
\begin{tikzpicture}[
    node distance=1.7cm and 2.2cm,
    box/.style={
        rectangle, rounded corners, draw=black, fill=green!15, 
        text width=3.4cm, align=center, minimum height=0.9cm,
        font=\small
    },
    arrow/.style={->, thick}
]

% Nodes
\node[box] (users) {Users \\[3pt]\footnotesize Provide ratings and demographic info};
\node[box, right=of users] (obtain) {Obtain User Information \\[3pt]\footnotesize Collect rating history and features};
\node[box, right=of obtain] (builduser) {Build User Models \\[3pt]\footnotesize Average feature vectors of liked books};
\node[box, right=of builduser] (update) {Update Models \\[3pt]\footnotesize Adjust profiles with new ratings};

\node[box, below=of builduser] (feedback) {Feedback or Interest Change \\[3pt]\footnotesize User interactions refine profiles};

\node[box, below=of obtain] (buildobject) {Build Object Feature Models \\[3pt]\footnotesize Encode books with TF–IDF + numeric data};
\node[box, right=of buildobject] (similarity) {Similarity Comparison \\[3pt]\footnotesize Cosine similarity between user and book vectors};
\node[box, right=of similarity] (history) {Personalized Information \\[3pt]\footnotesize Combine past preferences with results};
\node[box, below=of history] (results) {Provide Recommendation Results \\[3pt]\footnotesize Return top-N most similar books};

% Arrows
\draw[arrow] (users) -- (obtain);
\draw[arrow] (obtain) -- (builduser);
\draw[arrow] (builduser) -- (update);
\draw[arrow] (builduser) -- (feedback);
\draw[arrow] (update) |- (similarity);
\draw[arrow] (feedback) -| (update);
\draw[arrow] (buildobject) -- (similarity);
\draw[arrow] (similarity) -- (history);
\draw[arrow] (history) -- (results);
\draw[arrow] (results) -| (users);

\end{tikzpicture}
}
\caption{Flowchart of the Content-Based Filtering process with steps implemented in the book recommender system.}
\label{fig:cbf-flowchart}
\end{figure}


\subsection{Item-Based Collaborative Filtering vs. Content-Based Filtering}

Although both methods aim to recommend items similar to those a user already liked, they differ in what information they rely on and how they define similarity.

\textbf{Item-Based Collaborative Filtering (CF)} focuses on \textit{user behavior patterns}.  
Items are considered similar if many users have interacted with both of them in similar ways.  
For example, if several users who bought \textit{Book A} also bought \textit{Book B}, the system concludes that these two books are related, even if their topics are entirely different.  
This method depends on user ratings or implicit feedback (such as clicks or purchases) to measure similarity between items.  
In essence, the approach follows the idea: \textit{``Users who liked this item also liked that item.''}

\textbf{Content-Based Filtering (CBF)}, on the other hand, focuses on \textit{the characteristics of the items themselves}.  
It recommends new items similar to those the same user has already liked, based on item features such as genre, keywords, author, or description.  
For instance, if a user enjoys a mystery novel with a strong female lead, the system suggests other mystery novels with similar themes or features.  
The guiding principle is: \textit{``You liked this because of its content, so you might like similar content.''}

\subsection{Advantages and Disadvantages of Collaborative and Content-Based Filtering}

\textbf{Collaborative Filtering:}  
\textbf{Advantages:} Collaborative filtering can uncover new items that a user may not have considered before by leveraging the preferences of similar users. This allows for more diverse recommendations that go beyond the user’s past interactions.  

\textbf{Disadvantages:} Collaborative filtering suffers from the \textit{cold-start problem}. New users have no interaction history, making it difficult to find similar users, and new items lack sufficient ratings to be recommended effectively. Additionally, calculating similarities between users can be computationally expensive for large datasets.

\textbf{Content-Based Filtering:}  
\textbf{Advantages:} Content-based filtering handles new items efficiently, as recommendations rely on item features rather than prior user interactions. It also provides greater transparency, explaining why a particular item is recommended. For example, a movie may be suggested because it shares the same genre or actors as previously liked movies, allowing users to make informed decisions.  

\textbf{Disadvantages:} Content-based filtering is limited by the features available to describe items. If a user’s preference is based on characteristics not captured in the item profile (e.g., specific plot elements or production details), the system may fail to provide relevant recommendations. Moreover, it tends to overspecialize, suggesting items very similar to those already liked, which reduces diversity and limits discovery of new or unexpected items.

\section{Similarity/Distance Measures}\label{sec:similarity}

In content-based filtering, items are compared using their feature representations, and those that are closer in feature space are
considered more similar. In collaborative filtering, similarity is computed between users or items based on their interaction or rating
patterns, with closer vectors indicating stronger similarity. Both approaches use metrics like cosine similarity or correlation to
quantify closeness.

\subsection{Cosine Similarity}\label{sec:cosine}

One of the most common methods, especially for high-dimensional feature spaces, is \textbf{Cosine Similarity}. Cosine similarity measures
cosine of the angle between two vectors, giving a value between -1 and 1. The closer the value is to 1, the more similar the items are considered. 

\begin{center}
\begin{tikzpicture}[scale=5]

% Axes
\draw[->, thick] (0,0) -- (1.5,0) node[anchor=north] {Book Feature 1 (Drama)};
\draw[->, thick] (0,0) -- (0,1.5) node[anchor=east] {Book Feature 2 (Romance)};

% Define vectors
\coordinate (X) at (1,0.8);
\coordinate (Y) at (0.8,1);

% Draw book vectors
\draw[->, thick, blue] (0,0) -- (X) node[midway, above right] {Book X};
\draw[->, thick, red] (0,0) -- (Y) node[midway, above left] {Book Y};

% Dashed projections to axes for Book X
\draw[dashed] (X) -- (X|-0,0); % to x-axis
\draw[dashed] (X) -- (0,0|-X); % to y-axis

% Dashed projections to axes for Book Y
\draw[dashed] (Y) -- (Y|-0,0); % to x-axis
\draw[dashed] (Y) -- (0,0|-Y); % to y-axis

% Compute angles of vectors
\pgfmathsetmacro{\angleX}{atan2(0.8,1)} % vector X
\pgfmathsetmacro{\angleY}{atan2(1,0.8)} % vector Y

% Draw angle arc between vectors
\draw[thick] (0,0) +(\angleX:0.3) arc[start angle=\angleX, end angle=\angleY, radius=0.3];
\node at (0.18,0.22) {$\theta$};

\end{tikzpicture}
\end{center}

Here, $\theta$ is the angle between vectors $\mathbf{x}$ and $\mathbf{y}$. Cosine similarity measures how aligned these vectors are: the smaller the angle, the closer the similarity is to 1.

\textbf{Intuition:} Cosine similarity focuses on the direction of vectors rather than their magnitude. Two items with similar feature patterns
 are considered similar, even if one has generally higher values than the other.

The formula for cosine similarity between two item vectors $\mathbf{x}$ and $\mathbf{y}$ is:

\begin{equation}
\text{sim}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \, \|\mathbf{y}\|} 
= \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \; \sqrt{\sum_{i=1}^{n} y_i^2}}
\end{equation}

Here, $\mathbf{x} \cdot \mathbf{y}$ represents the dot product of the two vectors, where $\mathbf{x}$ and $\mathbf{y}$ are vectors, for example,
item feature vectors in a recommendation system and $\|\mathbf{x}\|$ and $\|\mathbf{y}\|$ are the magnitudes (lengths) of the vectors.
This metric allows the system to quantify similarity between items and recommend those that are most closely aligned with the user's
previous preferences.

In content-based filtering, each item is represented as a vector in a multi-dimensional feature space. For example, a book could be represented as
\[
\mathbf{x} = [\text{drama score}, \text{romance score}, \text{adventure score}, \dots].
\]

\begin{itemize}
    \item The \textbf{dot product} of two vectors $\mathbf{x}$ and $\mathbf{y}$ quantifies how much the vectors point in the same direction:
    \begin{equation}
    \mathbf{x} \cdot \mathbf{y} = x_1 y_1 + x_2 y_2 + \dots + x_n y_n = \sum_{i=1}^{n} x_i y_i
    \end{equation}
    Each term represents the contribution of a feature to the overall similarity.
    \item The \textbf{norm} or length of a vector is:
    \begin{equation}
    \|\mathbf{x}\| = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}
    \end{equation}
    Normalizing by the vector lengths ensures that cosine similarity measures direction rather than magnitude.
\end{itemize}

Cosine similarity is then defined as:
\begin{equation}
\text{sim}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \, \|\mathbf{y}\|} = \cos \theta
\end{equation}

\noindent where $\theta$ is the angle between the vectors. The similarity ranges from $-1$ (opposite) to $1$ (identical), with $0$ indicating orthogonal (no similarity).

\subsubsection{Euclidean Distance}

Euclidean distance is a way to measure how far apart two items are in a multi-dimensional feature space. It is the length of the straight line connecting the two points representing the items. In recommendation systems, a smaller Euclidean distance between two items indicates that they are more similar in terms of their features.

Suppose we represent two books as feature vectors:
\[
\mathbf{x} = (\text{drama score}, \text{romance score}, \dots), \quad
\mathbf{y} = (\text{drama score}, \text{romance score}, \dots)
\]

For \(n\) features, the Euclidean distance between \(\mathbf{x}\) and \(\mathbf{y}\) is:

\begin{equation}
d(\mathbf{x}, \mathbf{y}) = \sqrt{ (\text{drama}_x - \text{drama}_y)^2 + (\text{romance}_x - \text{romance}_y)^2 + \dots }
= \sqrt{ \sum_{i=1}^{n} (x_i - y_i)^2 }
\end{equation}

\begin{itemize}
    \item Each term \((x_i - y_i)^2\) measures the squared difference in a single feature (e.g., drama, romance) between the two books.
    \item Summing over all features gives a combined measure of difference across all dimensions.
    \item Taking the square root converts this sum into the actual straight-line distance.
\end{itemize}

\textbf{Intuition:}  
- Two books with similar ratings across all features will have a **small Euclidean distance**.  
- Books with very different ratings in one or more features will have a **larger distance**.  

\medskip

\begin{center}
\begin{tikzpicture}[scale=2]

% Axes
\draw[->, thick] (0,0) -- (4,0) node[right] {Drama};
\draw[->, thick] (0,0) -- (0,4) node[above] {Romance};

% Points representing books
\coordinate (X) at (1.5,2.5);
\coordinate (Y) at (3,1);

% Draw points
\filldraw[blue] (X) circle (2pt) node[above left] {Book X};
\filldraw[red] (Y) circle (2pt) node[below right] {Book Y};

% Draw Euclidean distance
\draw[dashed, thick] (X) -- (Y) node[midway, above, sloped] {$d(\mathbf{x},\mathbf{y})$};

% Dotted projections for Book X
\draw[dotted, gray] (X) -- (X|-0,0); % horizontal to x-axis
\draw[dotted, gray] (X) -- (0,0|-X); % vertical to y-axis

% Dotted projections for Book Y
\draw[dotted, gray] (Y) -- (Y|-0,0); % horizontal to x-axis
\draw[dotted, gray] (Y) -- (0,0|-Y); % vertical to y-axis

\end{tikzpicture}
\end{center}


This diagram shows:
\begin{itemize}
    \item Each book as a point in a 2D feature space (Drama vs Romance).  
    \item The dashed line representing the Euclidean distance between the books.  
    \item Dotted lines showing the projections to each feature axis for clarity.  
\end{itemize}


\subsection{Pearson Correlation}\label{sec:pearson}

Pearson correlation measures the **linear relationship** between two items across multiple features. Unlike cosine similarity or Euclidean distance, Pearson correlation **removes the effect of magnitude** by centering each vector on its mean. This makes it useful when we care about **rating patterns** rather than absolute values.

\paragraph{Definition}

Given two books represented as feature vectors:
\[
\mathbf{x} = [\text{drama}_x, \text{romance}_x, \dots], \quad
\mathbf{y} = [\text{drama}_y, \text{romance}_y, \dots]
\]

Let \(\bar{x}\) and \(\bar{y}\) be the mean values of the features of each book:
\[
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i, \quad
\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i
\]

The Pearson correlation is:

\begin{equation}
\text{corr}(\mathbf{x}, \mathbf{y}) =
\frac{\sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})}
     {\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \; \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}
\end{equation}

\begin{itemize}
    \item Each feature is **mean-centered**: we subtract the book's average feature value.  
    \item The numerator computes the **covariance** between the two books' features.  
    \item The denominator normalizes by the magnitude of the mean-centered vectors, giving a value between \(-1\) and \(1\).  
\end{itemize}

\paragraph{Interpretation}

\begin{itemize}
    \item \(+1\) → features vary together perfectly (strong positive correlation).  
    \item \(0\) → no linear relationship between features.  
    \item \(-1\) → features vary in opposite directions (strong negative correlation).  
\end{itemize}

\medskip

\textbf{Example in 2D feature space: Drama vs Romance}

\begin{center}
\begin{tikzpicture}[scale=2]

% Axes
\draw[->, thick] (0,0) -- (4,0) node[right] {Drama};
\draw[->, thick] (0,0) -- (0,4) node[above] {Romance};

% Original book vectors
\coordinate (X) at (1.5,2.5);
\coordinate (Y) at (3,1);

% Draw points
\filldraw[blue] (X) circle (2pt) node[above left] {Book X};
\filldraw[red] (Y) circle (2pt) node[below right] {Book Y};

% Mean-centered vectors (shifted to origin)
\coordinate (Xc) at (X);
\coordinate (Yc) at (Y);

% Draw dashed line connecting mean-centered vectors
\draw[dashed, thick] (Xc) -- (Yc) node[midway, above, sloped] {covariance component};

% Draw axes from mean
\draw[dotted, gray] (Xc) -- (Xc|-0,0); % horizontal projection
\draw[dotted, gray] (Xc) -- (0,0|-Xc); % vertical projection
\draw[dotted, gray] (Yc) -- (Yc|-0,0); % horizontal projection
\draw[dotted, gray] (Yc) -- (0,0|-Yc); % vertical projection

\end{tikzpicture}
\end{center}

\textbf{Notes:}  
\begin{itemize}
    \item The vectors are **mean-centered** by subtracting each book's average feature value.  
    \item The dashed line visually represents the **co-movement of features** contributing to Pearson correlation.  
    \item This method measures **pattern similarity**, so two books with similar relative features but different absolute levels can still have a high correlation.
\end{itemize}

\section{Evaluation Metrics}\label{sec:evaluation}

To assess the performance of the content-based recommender system, we employ two categories of evaluation metrics: 
(1) prediction accuracy metrics, which measure how accurately the system predicts user ratings, and 
(2) top-N recommendation metrics, which evaluate the quality of the ranked lists of recommended books.

    \subsection{Prediction Accuracy Metrics}
    These metrics evaluate how close the predicted ratings $\hat{r}_{ui}$ are to 
    the actual user ratings $r_{ui}$.

    \begin{itemize}
        \item \textbf{Mean Absolute Error (MAE)}
        \begin{equation}
        MAE = \frac{1}{|T|} \sum_{(u,i) \in T} |r_{ui} - \hat{r}_{ui}|
        \end{equation}
        where $T$ denotes the test set of user--item pairs. Lower MAE indicates higher predictive accuracy.

        \item \textbf{Root Mean Squared Error (RMSE)}
        \begin{equation}
        RMSE = \sqrt{ \frac{1}{|T|} \sum_{(u,i) \in T} (r_{ui} - \hat{r}_{ui})^2 }
        \end{equation}
        RMSE penalizes larger errors more heavily than MAE, and thus reflects how severely the system deviates from the actual ratings.

        \item \textbf{Mean Squared Error (MSE)}
        \begin{equation}
        MSE = \frac{1}{|T|} \sum_{(u,i) \in T} (r_{ui} - \hat{r}_{ui})^2
        \end{equation}
        MSE serves as the base metric for RMSE, providing a measure of the average squared deviation between predicted and true ratings.

        \item \textbf{Normalized Mean Absolute Error (NMAE)}
        \begin{equation}
        NMAE = \frac{MAE}{r_{\text{max}} - r_{\text{min}}}
        \end{equation}
        NMAE normalizes the MAE by the range of the rating scale, enabling comparability across datasets with different rating intervals.
    \end{itemize}

    \subsection{Top-N Recommendation Metrics}
    When the system produces a ranked list of recommended items, ranking-based metrics are preferred over pure rating prediction metrics.

    \begin{itemize}
        \item \textbf{Precision@N}
        \begin{equation}
        Precision@N = \frac{\text{Number of relevant items in top } N}{N}
        \end{equation}
        Precision@N measures the proportion of relevant items among the top-$N$ recommendations.

        \item \textbf{Recall@N}
        \begin{equation}
        Recall@N = \frac{\text{Number of relevant items in top } N}{\text{Total number of relevant items}}
        \end{equation}
        Recall@N evaluates how well the system retrieves all relevant items for a user, indicating the completeness of recommendations.
    \end{itemize}

\section{Deployment of Book Recommendation System}

Having an app on our local system simply isn't enough, it needs to be accessible online.
Deployment is the process of making your application available for users so they can access and interact with it.
It usually means moving your code from a development environment to a live production environment, where the application is fully operational.


\begin{enumerate}
    \item \textbf{User Input:} The user types a book title (and optionally a user ID) in the application interface.
    \item \textbf{User Interface / Front END:} The user using frontent sends a request and formats it into an API request (e.g., JSON) that is sent to the API.
    \item \textbf{API:} The API acts as a gateway. It receives the request, validates it, and forwards it to the server. It also ensures that only necessary information is exchanged, keeping internal details secure.
    \item \textbf{Server / Client APP / Backend:} The server runs the recommendation logic:
        \begin{itemize}
            \item Looks up the book embedding from the Oracle database or computes it if it’s free text.
            \item Performs a nearest-neighbor search in the embedding space to find similar books.
            \item Optionally combines item similarity with user profile data for personalization.
        \end{itemize}
    \item \textbf{Oracle Database:} Stores all persistent data, including books, embeddings, and user interaction history. The server queries it using SQL, retrieves the necessary data, and processes it in memory.
    \item \textbf{Response:} After computing the Top-N recommended books, the server sends the results back to the API, which forwards them to the client app. The user sees the recommendations seamlessly in the interface.
\end{enumerate}


\begin{center}
\begin{tikzpicture}[
  box/.style={draw, rounded corners, minimum width=2.5cm, minimum height=0.8cm, align=center, fill=#1!30, text centered},
  arrow/.style={-{Stealth[length=2.5mm]}, thick, color=#1},
  node distance=1.5cm
]

% Top row nodes
\node[box=blue] (user) {User \\ (types book name)};
\node[box=green, right=of user] (client) {User Interface \\ (sends request)};
\node[box=orange, right=of client] (api) {API \\ (handles request/response)};
\node[box=red, right=of api] (server) {Server / Client App \\ (runs recommendation model)};
\node[box=cyan, below=1.5cm of server] (db) {Oracle Database \\ (stores books, embeddings, user data)};
\node[box=purple, below=1.5cm of api] (response) {Response \\ (recommended books)};

% Top arrows (request)
\draw[arrow=blue] (user) -- (client);
\draw[arrow=green] (client) -- (api);
\draw[arrow=orange] (api) -- (server);
\draw[arrow=red] (server.south) -- (db.north);

% Database response to server
\draw[arrow=cyan] (db.north) -- ++(0,0.5) -| (server.south);

% Response arrows back to client/user
\draw[arrow=purple] (server.south east) -- (response.north west);
\draw[arrow=purple] (response.north east) -- (api.south);
\draw[arrow=orange] (api.west) -- ++(-0.9,0) |- (client.south);
\draw[arrow=green] (client.north) -- ++(-0.9,0) |- (user.south);

\end{tikzpicture}
\end{center}

\begin{itemize}
    \item \textbf{User $\rightarrow$ User Interface $\rightarrow$ API $\rightarrow$ Server/Client App $\rightarrow$ Database:} This is the request path.
    \item \textbf{Database $\rightarrow$ Server/Client App $\rightarrow$ API $\rightarrow$ User Interface $\rightarrow$ User:} This is the response path.
    \item The API acts as a secure bridge and ensures that internal server and database logic is not exposed to the client or user.
    \item The Oracle database stores all persistent data, and the server retrieves only what is needed for computing recommendations.
    \item The server performs the actual recommendation computation, including vector lookups, nearest-neighbor searches, and optional personalization.
\end{itemize}


\subsection{Application Programming Interface (API)}

An API, short for Application Programming Interface, is basically a bridge that lets different software programs talk to each other.
It defines a set of rules and protocols that allow applications to exchange data, features, or functionality. 
Instead of building every feature from scratch, developers can use APIs to connect their apps with existing services or data sources,
saving time and effort.
APIs also give application owners a controlled and secure way to share certain parts of their app's data or capabilities—either with
internal teams or with outside users. They're designed to expose only the specific information or actions that are needed for a given 
task, not the entire system. By doing this, APIs help protect sensitive information and maintain overall system security while still
enabling smooth, efficient communication between different software systems.

It helps to think of API communication as a conversation between a client and a server. In a book recommender system,
 the user's app—the client—sends a request, and the server responds with recommendations.
  The API is the bridge that makes this connection possible.

Here's how it works in practice: imagine a user types in the name of a book they like. When they hit “Submit,” 
the app sends this information as a request through the API. 
The request travels to the server, which runs a trained recommendation model to find books similar to the one the user entered.

The server then sends back a response containing a list of recommended books. 
The API takes this data and delivers it to the app, so the user sees their personalized recommendations almost instantly.

From the user's perspective, it feels seamless—they just type a book name and get suggestions.
 Behind the scenes, though, the API is carefully handling the request and response, sharing only the information needed, 
 keeping the system secure, and making sure the app and server communicate smoothly.


\section{Exploratory Data Analysis (EDA) and Data Preparation}

We begin by loading our datasets and cleaning column names for consistency.
We remove leading/trailing spaces, convert all names to lowercase, and replace dashes (\texttt{-})
with underscores (\texttt{\_}). This makes the DataFrame easier to work with in subsequent analysis.


\subsection{Books Dataframe}

Our books dataset contains several key columns:  
\textbf{isbn} (unique book identifier), \textbf{book\_title} (title of the book), \textbf{book\_author} (author name), \textbf{year\_of\_publication} (publication year), \textbf{publisher} (publisher name), and \textbf{image\_url\_s/m/l} (small, medium, and large book cover images). For analysis, the image columns (\texttt{image\_url\_s}, \texttt{image\_url\_m}, \texttt{image\_url\_l}) are dropped as they are not needed.  
All remaining attributes are of type \texttt{object}.

The dataset contains \textbf{271,360 book entries}. Key observations include:

\begin{itemize}
  \item \textbf{\texttt{isbn}} \- Every book has a unique ISBN, making it a perfect \textbf{unique identifier}.
  \item \textbf{\texttt{book\_title}} \- There are \textbf{242,135 unique titles} (about \textbf{89\%} of entries). Most titles are unique, though some duplicates exist, likely due to \textit{different editions or reissues}. For example, \textit{Selected Poems} appears 27 times with different ISBNs.
  \item \textbf{\texttt{book\_author}} \- Around \textbf{102,022 unique authors} are present, meaning each author has, on average, about \textbf{2–3 books}.
  \item \textbf{\texttt{publisher}} \- Only \textbf{16,807 unique publishers} appear (roughly \textbf{6\%} of entries), which is expected since many books come from the same publisher. \textit{Harlequin} is the most common.
  \item \textbf{\texttt{year\_of\_publication}} \- There are just \textbf{202 distinct years}, typical because many books share the same publication year. \textbf{2002} is the most frequent year, appearing in 13,903 books.
\end{itemize}

\noindent
Overall, the dataset is consistent: \textbf{ISBNs are unique}, while other fields naturally repeat due to multiple books by the same authors, publishers, or publication years.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/books_df/unique_values.png} % path to your image
    \caption{Unique Values Books DataFrame}
    \label{fig:books_df_unique}
\end{figure}

All book related columns are mostly complete. The book author and publisher columns each have only 2 missing values out of 271,360 entries, while all other columns are fully populated.  

There are two books with missing authors: "A+ Quiz Masters:01 Earth" (ISBN 0751352497)
and "The Credit Suisse Guide to Managing Your Personal Finances" (ISBN 9627982032).
We looked up the correct author information online and filled them as "Dorling Kindersley Publishing Staff"
and "Credit Suisse," respectively.  

Similarly, two books had missing publishers: "Tyrant Moon" (ISBN 193169656X) and "Finders Keepers" (ISBN 1931696993).
We searched for the correct publishers and updated them to "Mundania Press LLC" and "Random House Publishing Group."  

After these corrections, all books now have complete information for both authors and publishers.

We further explored the dataset by plotting value counts and found that the authors
with the most published books are Agatha Christie (632), William Shakespeare (567),
Stephen King (524), Ann M. Martin (423), and Carolyn Keene (373). The publishers with
the most books are Harlequin (7,535), Silhouette (4,220), Pocket (3,905), and Ballantine
Books (3,783).

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/books_df/top_author.png}
        \caption{Author with most published books}
        \label{fig:top_author}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/books_df/top_publisher.png}
        \caption{Publisher with most published books}
        \label{fig:top_publisher}
    \end{minipage}
\end{figure}

The \texttt{year\_of\_publication} column is quite messy, containing a mix of valid and invalid entries.
It includes integers like 1999 or 2002, strings that represent years such as "2000" or "1998",
non year strings like publisher names ("DK Publishing Inc", "Gallimard"), and impossible or unrealistic
years such as 0, 1378, 1806, 2030, and 2050.

Interestingly, only a few rows contain genuinely non-numeric entries, even though there are over
65,000 string values. Most of these strings are actually numeric ("1998", "2003", "0") and will 
need to be converted to integers and cleaned before analysis. 

To clean this column, we can convert all values to numeric using \texttt{pd.to\_numeric(errors='coerce')},
which turns invalid entries into \texttt{NaN}. Then, we replace unrealistic years (less than 1000 or greater than 2025)
with \texttt{NaN}, fill missing values with the median, and finally convert the column to integers. This ensures that \texttt{year\_of\_publication} 
s consistent, numeric, and ready for analysis.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/books_df/historgram_publishing_age_from_from_1000.png} % path to your image
    \caption{Histogram of Publication age}
    \label{fig:publication_age}
\end{figure}

We can also notice from the histogram that very few books in our dataset were published before 1900, 
so we can remove those values and keep only books published from 1900 onwards.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/books_df/historgram_publishing_age_from_from_1900.png} % path to your image
    \caption{Histogram of Publication age from 1900}
    \label{fig:publication_age_1900}
\end{figure}


\subsection{Ratings Dataframe}

Our ratings dataset contains three columns: \textbf{user\_id}, \textbf{isbn}, and \textbf{book\_rating}, with approximately 1,149,780 entries. 
There are no missing values. The \textbf{user\_id} column is just an identifier, so statistics like mean or std are not meaningful. 
The \textbf{book\_rating} column ranges from 0 (book read but not rated) to 10, with a mean of 2.87 and a standard deviation of 3.85,
indicating high variability. About 50\% of ratings are 0 and 75\% are below 7, showing a sparse ratings matrix typical of recommendation datasets. 

\textbf{Data types:} \textit{isbn} is categorical, \textit{user\_id} and \textit{book\_rating} are integers.  
\textbf{Unique values:} isbn: 340,556 (\(\sim\)30\%), user\_id: 105,283 (\(\sim\)9\%), book\_rating: 11 (0--10).

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/ratings_df/ratings_df_unique_values.png} % path to your image
    \caption{Unique Values Ratings DataFrame}
    \label{fig:ratings_df_unique}
\end{figure}

Many books were rated by multiple users, and many users rated multiple books.

The histogram shows the distribution of user ratings for books. A large number of users (716,109)
did not provide a rating, which reflects real life behavior, most people enjoy a product and move
on without leaving an online review. While this is natural, it poses a challenge for collaborative
recommendation systems, as unrated items result in zero-length vectors and sparse data. For user-based 
filtering, new users with few or no ratings make it hard to find similar users. For item-based filtering,
new books with few ratings are difficult to recommend, leading to the cold start problem and sparse data
issues.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/books_df/ratings_histogram.png}
        \caption{Histogram of Ratings given}
        \label{fig:ratings_gives}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/books_df/ratings_with_values.png}
        \caption{Histogram of Ratings given with values}
        \label{fig:ratings_with_values}
    \end{minipage}
\end{figure}

Our ratings dataset contains no missing values.

\subsection{Users Dataframe}

We start by preprocessing the \texttt{location} column in the user dataset. User-entered locations are often messy: typos, alternate spellings, subregions, or single-entry countries. Our goal is to standardize and simplify these values.

\begin{itemize}
  \item \textbf{split\_location(df):} Splits the \texttt{location} column into \texttt{city}, \texttt{state}, and \texttt{country}, trims whitespace, and fills missing values with \texttt{'unknown'}.
  \item \textbf{clean\_country(df, country\_mapping, region\_mapping):} Standardizes country names, groups rare countries into \texttt{'other'}, removes quotes/empty strings, maps each country to a \texttt{region}, and drops the original \texttt{country} column.
  \item \textbf{clean\_city(df, top\_n=50):} Cleans city names, replaces invalid/missing entries with \texttt{'unknown'}, keeps only the top N frequent cities, labels the rest as \texttt{'other'}, and drops the original column.
  \item \textbf{clean\_state(df, top\_n=50):} Similar to \texttt{clean\_city}, but for states.
  \item \textbf{preprocess\_location(df, ...):} Runs all the above steps and returns cleaned columns: \texttt{city\_clean}, \texttt{state\_clean}, \texttt{country\_clean}, and \texttt{region}.
\end{itemize}

Overall, this process transforms messy location data into standardized, manageable categories suitable for analysis or modeling.

The dataset contains \textbf{278,858 users}, but only \textbf{168,096 have a recorded age}, meaning roughly \textbf{110,000 users did not provide their age}.  

The \texttt{user\_id} column is fully unique and serves as a reliable identifier. Its average value (\textasciitilde 139,429) is not meaningful.  
The \texttt{age} column has an average of approximately \textbf{35 years} with a standard deviation of about \textbf{14 years}, indicating a fairly wide spread.

Examining the age distribution:
\begin{itemize}
  \item The \textbf{youngest recorded age is 0}, which is clearly invalid or missing.  
  \item \textbf{25\% of users are younger than 24}, and \textbf{50\% are younger than 32} (median).  
  \item \textbf{75\% are younger than 44}, meaning most users fall between their mid-20s and mid-40s.  
  \item The \textbf{maximum recorded age is 244}, an obvious outlier or error.
\end{itemize}

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/users_df/missing_values.png} % path to your image
    \caption{Missing Values Users DataFrame}
    \label{fig:missing_values_users_df}
\end{figure}

Regarding column uniqueness:
\begin{itemize}
  \item \texttt{user\_id} is fully unique (100\%), serving as a unique identifier.  
  \item \texttt{age} has 165 unique values (\textasciitilde 0.06\%), highlighting the presence of outliers.  
  \item \texttt{country\_clean} has 131 unique entries (\textasciitilde 0.05\%), \texttt{city\_clean} 51 (\textasciitilde 0.02\%), and \texttt{state\_clean} 50 (\textasciitilde 0.02\%).  
  \item \texttt{region} is the least varied, with only 13 unique values (\textasciitilde 0.00\%).
\end{itemize}

Overall, the age data contains some missing and unrealistic values, but most users are in the \textbf{24 \- 44 year range}, and all other columns are largely complete and consistent.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/users_df/unique_values.png} % path to your image
    \caption{Unique Values Users DataFrame}
    \label{fig:unique_values_users_df}
\end{figure}

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/users_df/unique_without_id.png} % path to your image
    \caption{Unique Values Without ID Users DataFrame}
    \label{fig:unique_values_without_id_users_df}
\end{figure}

Most reader ratings come the USA leading at 139,599 ratings, followed by Canada (21,622), the United Kingdom (18,587), Germany (17,092), and Spain (13,313).

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/country_piechart.png}
        \caption{Piechart of countries of user's who gave ratings}
        \label{fig:piechar_country}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/country_barchart.png}
        \caption{Histogram of countries of user's who gave ratings}
        \label{fig:barchart_country}
    \end{minipage}
\end{figure}

Most users are grouped under the category 'Other' because, during preprocessing, all less-popular cities were combined into this category. Excluding 'Other', we can see that major cities dominate user ratings, with London (4,105), Barcelona (2,664), Toronto (2,342), Madrid (1,933), Sydney (1,884), Chicago (1,566), and New York (1,445) leading the list.


\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/piechart_city.png}
        \caption{Piechart of cities of user's who gave ratings}
        \label{fig:piechart_city}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/piechar_city_no_other.png}
        \caption{Piechar of cities of user's who gave ratings}
        \label{fig:piechart_city_no_other}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/barchart_city.png}
        \caption{Barchart of cities of user's who gave ratings}
        \label{fig:barchart_city}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/barchart_city_no_other.png}
        \caption{Barchart of cities of user's who gave ratings}
        \label{fig:barchart_city_no_other}
    \end{minipage}
\end{figure}

The \texttt{age} column in our dataset is \textbf{not normally (Gaussian) distributed}. 
It is heavily skewed, with many extreme values, which makes standard outlier detection methods,
such as the Interquartile Range (IQR), inadequate for this data. Which we can notice and compare with 
normal Gaussian distribution on following histograms:

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/age_histogram.png}
        \caption{Age Histogram}
        \label{fig:age_histogram}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/normal_distribution_histogram.png}
        \caption{Normal Gaussian distribution histogram}
        \label{fig:normal_histogram}
    \end{minipage}
\end{figure}

Using the IQR method:

\[
\text{IQR} = Q3 - Q1
\]

\begin{itemize}
    \item \textbf{Lower bound:} $Q1 - 1.5 \times \text{IQR} = 24 - 1.5 \times 30 = -6$  
    This is meaningless, as it would leave negative ages or zeros, which are invalid.
    \item \textbf{Upper bound:} $Q3 + 1.5 \times \text{IQR} = 44 + 1.5 \times 30 = 74$  
    This is too low, as it would incorrectly remove valid older users aged 75 \- 99.
\end{itemize}

The IQR method alone is insufficient for this dataset. Instead, explicit filtering is required to ensure realistic and meaningful values. We choose to remove ages below 5 and above 99, which preserves the majority of valid data while removing unrealistic outliers.

The \texttt{age} column contains 168,096 entries with a mean of 34.75 and a standard deviation of 14.43.
The values range from 0 to 244, with the 25th, 50th, and 75th percentiles at 24, 32, and 44, respectively.
The standard deviation, $\sigma$, measures how spread out the values are around the mean and is calculated as

\begin{equation}
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2}
\end{equation}

where $x_i$ represents each individual age, $\mu$ is the mean age, and $N$ is the total number of entries.
In this dataset, the high value of $\sigma = 14.43$ reflects the presence of many extreme outliers,
which is evident in boxplots where ages far beyond the typical range appear as points outside the whiskers.

For comparison, a standard normal (Gaussian) distribution with a mean around 50 and a standard 
deviation of 9.69 has values mostly concentrated between 17 and 84. Unlike this idealized normal distribution,
our \texttt{age} data is heavily skewed.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/boxplot_age.png}
        \caption{Age Boxplot}
        \label{fig:age_boxplot}
    \end{minipage}
    \hfill
    \begin{minipage}{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/boxplot_gaussian.png}
        \caption{Normal Gaussian distribution Boxplot}
        \label{fig:normal_boxplot}
    \end{minipage}
\end{figure}

After removing unrealistic ages below 5 and above 100, there are still a large number of missing values,
with 112,043 rows lacking an \texttt{age}. Dropping these rows would cause significant data loss,
so we impute them using a random method based on the existing age distribution. This approach uses the
median to center the distribution, the standard deviation to preserve the spread, and ensures that 
generated ages stay within the realistic range of 5 to 100. The imputed values are rounded to integers,
maintaining a discrete age variable. After imputation, the overall age distribution remains similar to 
the original data, no extreme outliers are introduced, and all ages fall within a plausible range.
The final histogram and boxplot confirm that the distribution looks realistic.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/age_histogram_final.png}
        \caption{Age Histogram Final}
        \label{fig:age_histogram_final}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/users_df/age_boxplot_final.png}
        \caption{Age Boxplot Final}
        \label{fig:age_boxplot_final}
    \end{minipage}
\end{figure}

\subsection{Merging datasets and Feature engineering}

We merge our datasets to continue cleaning and enriching the data, allowing us to create new features
and gain deeper insights. The \texttt{users\_df} (278,858 rows, 6 columns) is merged with the
\texttt{ratings\_df} (1,149,780 rows, 3 columns) on the \texttt{user\_id} column. Each resulting 
row represents a rating linked to the corresponding user's information. 

By default, \texttt{pd.merge()} performs an inner join, which keeps only the \texttt{user\_id}s that
exist in both datasets. Users without ratings, or ratings without matching user information, are excluded
from the result.

When merging with the books dataset, we join on the \texttt{isbn} column rather than
\texttt{book\_title}, since \texttt{isbn} uniquely identifies each book edition.
In contrast, \texttt{book\_title} is not unique, multiple editions may share the same title, 
and inconsistencies such as typos or casing differences can lead to duplicate rows, 
incorrect matches, or missing records. Joining on \texttt{isbn} ensures accuracy and consistency 
across datasets.

We can observe that the books with the most ratings and highest average scores include popular titles
such as *The Da Vinci Code*, *Harry Potter and the Sorcerer's Stone*, *To Kill a Mockingbird*, and *The Lovely Bones*. 
The overall average rating across all books is around 4.2. However, as seen earlier, many users gave a 
rating of 0. These represent readers who read the book but did not actually rate it. After removing all 0
ratings, only two books have more than 500 valid ratings: *The Lovely Bones* by Alice Sebold, with an impressive average 
score of 8.18, and *Wild Animus* by Rich Shapero, with an average score of 4.39.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/merged_df/zero_500.png}
        \caption{Most popular books with more than 500 ratings (0 ratings included)}
        \label{fig:zero_500}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/merged_df/no_zero_500.png}
        \caption{Most popular books with more than 500 ratings (NO 0 ratings included)}
        \label{fig:no_zero_500}
    \end{minipage}
\end{figure}

After removing all zero ratings, we can see which books received the most genuine user feedback. 
Among books with more than 300 ratings, popular titles such as *Harry Potter and the Sorcerer's Stone*, 
*The Da Vinci Code*, *Life of Pi*, and *The Lovely Bones* dominate the list, each with average scores 
above 8. These books are well-known bestsellers, suggesting a strong link between popularity and the
number of user ratings.

When we raise the threshold to 400 ratings, only four books remain: *The Da Vinci Code* by Dan Brown,
*The Lovely Bones* by Alice Sebold, *The Secret Life of Bees* by Sue Monk Kidd, and *Wild Animus*
by Rich Shapero. Interestingly, while the first three books maintain high average ratings around 8, 
*Wild Animus* stands out with a much lower score of 4.39, indicating that a large number of ratings 
does not always correlate with positive feedback.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/merged_df/no_zero_300.png}
        \caption{Most popular books with more than 300 ratings (NO 0 ratings included)}
        \label{fig:no_zero_300}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/merged_df/no_zero_400.png}
        \caption{Most popular books with more than 400 ratings (NO 0 ratings included)}
        \label{fig:no_zero_400}
    \end{minipage}
\end{figure}

\subsubsection{User Level Features}

We generate user-level features to better understand our dataset and capture user behavior.
These include the average rating each user gives (\texttt{user\_avg\_rating}), the total number of ratings
per user (\texttt{user\_num\_ratings}), and the consistency of their ratings measured by
the standard deviation (\texttt{User\_rating\_variability}). All these features are collected
in a separate dataframe for easier analysis. Examining the results, we notice that most users rated
only one book (39,223 users), with a rapid drop for two or three ratings, highlighting a significant
cold start problem that challenges user-based recommendation approaches. Nonetheless, some users are 
highly active, rating hundreds of books, which can still provide valuable patterns for generating 
recommendations.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/merged_df/users_rated_k_books.png} % path to your image
    \caption{How many users rated K books}
    \label{fig:users_rated_k_books}
\end{figure}

Also we can observe average ratings per user.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/merged_df/average_user_ratings.png} % path to your image
    \caption{Average rating per user}
    \label{fig:average_ratings_per_user}
\end{figure}

We create a user age group feature by dividing ages into five ranges:
0--18, 18--25, 25--35, 35--50, and 50--100, and assigning numeric codes from 1 to 5.
Since many users rate only one book and many books are rated by only one user, the standard deviation 
of ratings cannot be calculated for these cases. We replace these missing variability values with 0,
indicating no variation in their ratings.

\subsubsection{Book Level Features}

We create book-level features to better understand and analyze our dataset. 
These include the average rating for each book (\texttt{book\_avg\_rating}),
the total number of ratings per book (\texttt{book\_num\_ratings}), the variation
in book ratings measured by standard deviation (\texttt{book\_rating\_variability}),
and a popularity metric that combines average rating with the number of ratings (\texttt{book\_popularity\_score}). 
A summary dataframe is then generated containing these unique book-level features. Examining the number of ratings per book,
most books were rated only once (88,137 books), with the count dropping sharply for two or three ratings. This highlights the sparsity
of the dataset, which can make it harder for item-based collaborative filtering to find similar books. Nevertheless, 
some books received hundreds of ratings, with a few exceeding 500, showing that popular books can still provide strong signals for recommendations.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/merged_df/books_recieved_k_ratings.png} % path to your image
    \caption{How many books recieved K books}
    \label{fig:books_recieved_k_books}
\end{figure}

We analyze the distribution of average ratings per book to see how many books received each specific rating.
Most books cluster around high-range ratings, while a small amount of books achieve very low averages.
This gives insight into the overall perception of books in the dataset and helps identify highly rated or unpopular books.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/EDA/merged_df/ratings_distribution_book.png} % path to your image
    \caption{Books Average Rating Distribution}
    \label{fig:books_rating_distribution}
\end{figure}

\subsubsection{Location, Author, Publisher Level Features, Additional Plots and Insights}

We can extract additional features to gain deeper insights from our dataset.
This includes location based features such as the average rating per country (\texttt{country\_avg\_rating}), 
per region (\texttt{region\_avg\_rating}), and per city (\texttt{city\_avg\_rating}).
We also capture personal bias with \texttt{user\_country\_rating\_bias}, which measures the difference between a user's average rating and their country's average, 
and we track the local popularity of books using \texttt{book\_country\_avg\_rating}, the average rating of each book within each country.

Additional features are created at different levels, including author level features such as \texttt{author\_avg\_rating},
the average rating of all books by each author, and publisher level features such as \texttt{publisher\_avg\_rating},
the average rating of all books by each publisher. We also compute the age of each book (\texttt{book\_age}) as the difference between the current year (2025) and its year of publication, 
providing context for temporal analysis of ratings.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/merged_df/rating_popularity.png}
        \caption{Books average rating and book popularity}
        \label{fig:no_zero_300}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/merged_df/harry_potter_age.png}
        \caption{Harry Potter Ratings and User Age}
        \label{fig:harry_potter}
    \end{minipage}
\end{figure}

Heatmaps showing correlation between authors, publishers and user ages.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/merged_df/publisher_age_heatmap.png}
        \caption{Publisher user age heatmap}
        \label{fig:publisher_heatmap}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/EDA/merged_df/author_age_heatmap.png}
        \caption{Author user age heatmap}
        \label{fig:author_heatmap}
    \end{minipage}
\end{figure}

\section{Collaborative Filtering Models Building and Evaluation}

As explained in detail in \hyperref[sec:collaborative_filter]{\textbf{collaborative\_filter}}, 
this section implements the theoretical concepts discussed earlier. For collaborative filtering,
the model only requires the core user\-item interactions: \texttt{user\_id}, \texttt{isbn} or encoded book title, 
and \texttt{book\_rating}. Other metadata, such as user demographics or book averages, are not needed in standard CF,
although they can be incorporated in hybrid approaches.

CF works by capturing similarity patterns: users who rate similar books in similar ways, 
and books that are rated similarly by similar users. Columns like age, country, or \texttt{author\_avg\_rating} 
are considered side information. While these are not part of the core CF matrix, they can be useful for content based recommenders,
hybrid models combining CF with features, or for deeper analytics and fairness evaluations.

For a pure CF implementation, only the user\-item\-rating data is necessary. 
It is also important to filter out inactive users and books with very few ratings,
since new users and items lack sufficient interaction history to generate meaningful recommendations.

To reduce the cold start problem, we first select users who have rated at least 10 books and books that 
have received at least 10 ratings. After this initial filtering, some users or books may fall below the
threshold because removing books decreases the number of ratings for users, and removing users decreases
the number of ratings for books. This cascading effect requires iterative filtering: we repeatedly recompute
the counts and apply the thresholds until the dataset stabilizes. Initially, 6,589 users meet the threshold
of 10 ratings, and 5,712 books have been rated by at least 10 users. After applying iterative filtering,
we are left with 2,272 books and 2,508 users, as removing users and books successively eliminates some ratings
for the remaining items.

After applying the iterative filtering, the distribution of user activity and book ratings is still extremely
sparse. For users, only a small number have given many ratings: for example, 237 users rated exactly
10 books, 202 rated 11 books, 171 rated 12, and so on, with very few users contributing more than 100 
ratings. Similarly, for books, only a limited number have received many ratings: 289 books have exactly
10 ratings, 261 have 11, 203 have 12, and the counts rapidly decline for higher numbers, with some 
books receiving over 100 ratings but very few reaching the upper range. This highlights that even after
filtering, the dataset remains extremely sparse, making collaborative filtering challenging and emphasizing
the importance of focusing on the most active users and popular books.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Model_building/user_rated_k_books.png} % path to your image
    \caption{How many users have rated K books}
    \label{fig:users_gave_k_rratings_train}
\end{figure}

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Model_building/books_have_k_ratings.png} % path to your image
    \caption{How many books have recieved K rating}
    \label{fig:books_recieved_k_ratings_train}
\end{figure}

We first set up the data split by reserving 20\% of each user's ratings for testing, using 
a fixed random seed to ensure reproducibility. The dataframe is then grouped by user, and each 
user's ratings are shuffled randomly. For each user, the first portion of their ratings is assigned
to the test set, with the remainder going to the training set. If a user ends up with only one rating 
in either set, one rating is moved to the training set to ensure every user appears in both sets. 
The resulting \texttt{train\_df} contains all training ratings and \texttt{test\_df} contains all test ratings, 
with indices reset for cleanliness. This procedure guarantees that all users are represented 
in both train and test sets, allowing collaborative filtering models to be trained and evaluated 
reliably. The final print statements confirm the number of rows and unique users in each split.

We encode the \texttt{user\_id} and \texttt{book\_title} columns into numeric indices because collaborative filtering models require a user--item matrix with integer row and column indices. String IDs cannot be used directly in most matrix-based algorithms.

Using \textbf{label encoding}, each unique user and book in the training set is assigned a distinct integer, stored in \texttt{user\_idx} and \texttt{book\_idx}. The same mapping is then applied to the test set to ensure consistency, so the same user or book always has the same index across both sets.

After encoding, the number of unique users and books is printed, which should match the counts in the training set. This prepares the data for constructing the interaction matrix, where rows correspond to users, columns correspond to books, and entries represent ratings.

In short, this step transforms categorical IDs into integers, enabling the creation of the user--item matrix required for collaborative filtering.


\subsection{User Based CF}

We construct a sparse matrix $\mathbf{R} \in \mathbb{R}^{m \times n}$ using \texttt{csr\_matrix}, 
where each entry $R_{ui}$ represents the rating given by user $u$ to book $i$. 
Rows correspond to users and columns correspond to books, defining the dimensions of the matrix. 
Using a sparse matrix is memory efficient, since most users have rated only a small fraction of all books, 
leaving the matrix mostly empty.


In collaborative filtering, each user can be represented as a vector of their ratings.
The length of a user vector reflects the magnitude of their ratings: a user who gives 
many high ratings has a longer vector, while a user who gives few or low ratings has a 
shorter vector. Cosine similarity, however, ignores vector length and focuses on the
angle between vectors, capturing how similar the pattern of ratings is between users.
This allows identification of users with similar preferences across books, even if their
overall rating tendencies differ. By using top-K neighbors, we consider only the most similar
users when generating recommendations.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Model_building/user_user_cosine.png} % path to your image
    \caption{User User cosine similarity heatmap}
    \label{fig:user_user_cosine}
\end{figure}

Cosine similarity ranges from 0, indicating no similarity, to 1, indicating high similarity.
In the heatmap, darker colors correspond to low similarity (closer to 0), while lighter colors
indicate high similarity (closer to 1). Setting the diagonal to 0 removes self-similarity, ensuring 
that each user is not considered similar to themselves when identifying neighbors.

For each user, we first identify the top-$K$ most similar users from the user-user similarity matrix. 
These neighbors serve as the basis for generating recommendations, as their ratings provide insight into what the target user might like. 

To predict ratings, we calculate a weighted average of the neighbors' ratings for each user-item pair, 
using the similarity scores between the target user and each neighbor as weights. 
The resulting weighted sum is then normalized by the sum of similarity scores to ensure that the predicted ratings 
are scaled appropriately. If none of the neighbors has rated a given item, a fallback strategy can be used, 
such as the user's average rating or the global average rating across all users.

This process is repeated for every user and every item, producing a full prediction matrix. 
Each entry in this matrix represents the estimated rating that a particular user would give to a specific item, 
allowing the system to make personalized recommendations.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Model_building/user_book_predicted.png} % path to your image
    \caption{User Book rating prediction matrix}
    \label{fig:user_book_prediction}
\end{figure}

The evaluation metrics paint a brutally honest picture of the system's performance. 
Precision and recall indicate how well the system recommends books that users actually like, 
while MAE, MSE, and RMSE quantify how far the predicted ratings deviate from the true ratings.

Looking at the top-$k$ metrics, Precision@5 is only 0.0542. 
This means that out of the five books the system predicts a user will like most, 
on average, only about one in twenty is actually relevant. 
Even when increasing the recommendations to 20, Recall rises only to 0.1116, 
so the system captures barely 11\% of the items a user truly rated. 
Precision continues to drop as $k$ increases, indicating that recommending more items 
mostly adds irrelevant books. This performance is painfully low, though not surprising 
given the extreme sparsity typical of book rating datasets.

The rating prediction errors are equally harsh. 
A mean absolute error (MAE) of 7.3627 and a root mean squared error (RMSE) of 7.6219 
mean that, on average, the predicted rating is off by more than 7 points on a typical 1--10 scale. 
The mean squared error (MSE) of 58.0931 highlights that some predictions are catastrophically wrong. 
In practice, this suggests that the model is essentially guessing most of the time. 
While it may capture very rough trends, it is \textbf{terrible at predicting individual ratings} 
and fails to produce meaningful recommendations for real users.

In short, this model generates numbers but is nowhere near useful in practice. 
If deployed as-is, users would receive mostly irrelevant recommendations.

\subsection{Item Based CF}

To implement item-based collaborative filtering, we first transpose the original user-item rating matrix. 
In the user-item matrix, each row corresponds to a user and each column corresponds to a book. 
By transposing the matrix, we switch the axes so that each row now represents a book and each column represents a user. 

This transformation allows each book to be treated as a vector in the space of users, 
where the entries correspond to the ratings given by all users. 
Computing similarity between books then becomes a matter of comparing these vectors, 
typically using cosine similarity or another similarity metric. 

Formally, if $\mathbf{R} \in \mathbb{R}^{m \times n}$ is the original user-item matrix 
with $m$ users and $n$ books, we construct the item-user matrix $\mathbf{R}^\top \in \mathbb{R}^{n \times m}$ 
by taking the transpose. Each row $\mathbf{r}_i$ of $\mathbf{R}^\top$ is the rating vector for book $i$, 
allowing us to compute similarities between books and generate recommendations based on item-to-item relationships.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Model_building/book_book_cosine.png} % path to your image
    \caption{Item Item cosine similarity heatmap}
    \label{fig:item_item_cosine}
\end{figure}

Once the item-user matrix is constructed, we compute the similarity between books using cosine similarity. 
This results in an item-item similarity matrix, where each entry quantifies how similar two books are based on the ratings they received from all users.

For each book, we then identify the top-$K$ most similar books according to the similarity scores. 
These neighbors are used to generate predicted ratings for all users: 
for a given user and a target book, the predicted rating is calculated as a weighted average of the ratings that the user has given to the top-$K$ most similar books. 
The weights correspond to the similarity scores between the target book and each neighbor.

Repeating this process for every book and every user produces a full predicted rating matrix. 
Each entry in this matrix represents the estimated rating a particular user would give to a specific book, 
allowing the system to generate personalized recommendations based on item-to-item similarity rather than user-to-user similarity.

\begin{figure}[H] % 'h' means here, can also use t=top, b=bottom
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Model_building/item_based_prediction_matrix.png} % path to your image
    \caption{Item based prediction atrix heatmap}
    \label{fig:item_based_prediction}
\end{figure}

Examining the top-$N$ metrics, Precision@5 is only 0.0742, 
meaning that out of the five books recommended to a user, 
\textbf{less than one book on average is actually relevant}. 
Even when $N$ is increased to 20, the system captures only about 16\% of the relevant items in Recall@20. 
Precision decreases as more items are recommended, showing that most suggestions are irrelevant. 
This modest improvement over the user-based method indicates that item-based collaborative filtering 
captures some patterns in item similarity, but it is still extremely weak for meaningful recommendations. 
Sparse user ratings and limited overlap between items are major limiting factors.

The rating prediction errors remain severe. 
A mean absolute error (MAE) of 7.1474 and a root mean squared error (RMSE) of 7.4423 indicate that, 
on average, the predicted ratings are \textbf{off by more than 7 points}, which is substantial on a typical 1--10 scale. 
The mean squared error (MSE) of 55.3873 highlights that some predictions are catastrophically wrong. 
While the item-based approach slightly improves ranking of relevant items compared to user-based collaborative filtering, 
it still \textbf{fails to predict actual ratings accurately} and would produce mostly useless recommendations if deployed.

In summary, item-based collaborative filtering is only a small step forward. 
Most recommendations remain irrelevant, and predicted ratings are wildly inaccurate, 
making this method far from practical for real-world usage.

\section{Content Based Filtering}